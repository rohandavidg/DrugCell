{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/lambda_stor/homes/ac.tfeng/git/DrugCell'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import anndata\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List, Union, Optional, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.distributions import MultivariateNormal\n",
    "\n",
    "import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"4\"\n",
    "\n",
    "import copy\n",
    "\n",
    "import tqdm\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from codes.utils.util import *\n",
    "from codes.drugcell_NN import *\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "  DEVICE = 'cuda'\n",
    "else:\n",
    "  DEVICE = 'cpu'\n",
    "  \n",
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Dec 19 09:11:27 2023       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.104.12             Driver Version: 535.104.12   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  Tesla V100-SXM2-32GB           On  | 00000000:1A:00.0 Off |                    0 |\n",
      "| N/A   23C    P0              40W / 300W |      3MiB / 32768MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla V100-SXM2-32GB           On  | 00000000:1B:00.0 Off |                    0 |\n",
      "| N/A   24C    P0              40W / 300W |      3MiB / 32768MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   2  Tesla V100-SXM2-32GB           On  | 00000000:3D:00.0 Off |                    0 |\n",
      "| N/A   63C    P0             249W / 300W |   5059MiB / 32768MiB |     95%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   3  Tesla V100-SXM2-32GB           On  | 00000000:3E:00.0 Off |                    0 |\n",
      "| N/A   46C    P0             246W / 300W |   1835MiB / 32768MiB |     86%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   4  Tesla V100-SXM2-32GB           On  | 00000000:88:00.0 Off |                    0 |\n",
      "| N/A   23C    P0              40W / 300W |      3MiB / 32768MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   5  Tesla V100-SXM2-32GB           On  | 00000000:89:00.0 Off |                    0 |\n",
      "| N/A   26C    P0              42W / 300W |      3MiB / 32768MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   6  Tesla V100-SXM2-32GB           On  | 00000000:B2:00.0 Off |                    0 |\n",
      "| N/A   26C    P0              55W / 300W |   1507MiB / 32768MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   7  Tesla V100-SXM2-32GB           On  | 00000000:B3:00.0 Off |                    0 |\n",
      "| N/A   25C    P0              56W / 300W |  11199MiB / 32768MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    2   N/A  N/A   4026736      C   python                                     5056MiB |\n",
      "|    3   N/A  N/A   4027480      C   python                                     1832MiB |\n",
      "|    6   N/A  N/A    269207      C   .../miniconda3/envs/general/bin/python     1504MiB |\n",
      "|    7   N/A  N/A    269207      C   .../miniconda3/envs/general/bin/python    11196MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 'cuda:6'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GDSC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sample_idx = pd.read_csv(\"data/GDSC/GDSCv1_split_0_train.txt\")\n",
    "train_sample_idx = list(train_sample_idx.iloc[:,0])\n",
    "test_sample_idx = pd.read_csv(\"data/GDSC/GDSCv1_split_0_test.txt\")\n",
    "test_sample_idx = list(test_sample_idx.iloc[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_col_names_in_multilevel_dataframe(\n",
    "    df: pd.DataFrame,\n",
    "    level_map: dict,\n",
    "    gene_system_identifier: Union[str, List[str]]=\"Gene_Symbol\") -> pd.DataFrame:\n",
    "    \"\"\" Util function that supports loading of the omic data files.\n",
    "    Returns the input dataframe with the multi-level column names renamed as\n",
    "    specified by the gene_system_identifier arg.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): omics dataframe\n",
    "        level_map (dict): encodes the column level and the corresponding identifier systems\n",
    "        gene_system_identifier (str or list of str): gene identifier system to use\n",
    "            options: \"Entrez\", \"Gene_Symbol\", \"Ensembl\", \"all\", or any list\n",
    "                     combination of [\"Entrez\", \"Gene_Symbol\", \"Ensembl\"]\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: the input dataframe with the specified multi-level column names\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    level_names = list(level_map.keys())\n",
    "    level_values = list(level_map.values())\n",
    "    n_levels = len(level_names)\n",
    "    \n",
    "    if isinstance(gene_system_identifier, list) and len(gene_system_identifier) == 1:\n",
    "        gene_system_identifier = gene_system_identifier[0]\n",
    "\n",
    "    # print(gene_system_identifier)\n",
    "    # import pdb; pdb.set_trace()\n",
    "    if isinstance(gene_system_identifier, str):\n",
    "        if gene_system_identifier == \"all\":\n",
    "            df.columns = df.columns.rename(level_names, level=level_values)  # assign multi-level col names\n",
    "        else:\n",
    "            df.columns = df.columns.get_level_values(level_map[gene_system_identifier])  # retian specific column level\n",
    "    else:\n",
    "        assert len(gene_system_identifier) <= n_levels, f\"'gene_system_identifier' can't contain more than {n_levels} items.\"\n",
    "        set_diff = list(set(gene_system_identifier).difference(set(level_names)))\n",
    "        assert len(set_diff) == 0, f\"Passed unknown gene identifiers: {set_diff}\"\n",
    "        kk = {i: level_map[i] for i in level_map if i in gene_system_identifier}\n",
    "        # print(list(kk.keys()))\n",
    "        # print(list(kk.values()))\n",
    "        df.columns = df.columns.rename(list(kk.keys()), level=kk.values())  # assign multi-level col names\n",
    "        drop_levels = list(set(level_map.values()).difference(set(kk.values())))\n",
    "        df = df.droplevel(level=drop_levels, axis=1)\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_gene_expression_data(gene_expression_file_path, \n",
    "    gene_system_identifier: Union[str, List[str]]=\"Gene_Symbol\",\n",
    "    sep: str=\"\\t\",\n",
    "    verbose: bool=True) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Returns gene expression data.\n",
    "\n",
    "    Args:\n",
    "        gene_system_identifier (str or list of str): gene identifier system to use\n",
    "            options: \"Entrez\", \"Gene_Symbol\", \"Ensembl\", \"all\", or any list\n",
    "                     combination of [\"Entrez\", \"Gene_Symbol\", \"Ensembl\"]\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: dataframe with the omic data\n",
    "    \"\"\"\n",
    "    # level_map encodes the relationship btw the column and gene identifier system\n",
    "    level_map = {\"Ensembl\": 0, \"Entrez\": 1, \"Gene_Symbol\": 2}\n",
    "    header = [i for i in range(len(level_map))]\n",
    "\n",
    "    df = pd.read_csv(gene_expression_file_path, sep=sep, index_col=0, header=header)\n",
    "\n",
    "    df.index.name = \"improve_sample_id\"  # assign index name\n",
    "    df = set_col_names_in_multilevel_dataframe(df, level_map, gene_system_identifier)\n",
    "    if verbose:\n",
    "        print(f\"Gene expression data: {df.shape}\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gene expression data: (1007, 30805)\n"
     ]
    }
   ],
   "source": [
    "gene_express = load_gene_expression_data(\"data/GDSC/cancer_gene_expression.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/homes/ac.tfeng/miniconda3/envs/general/lib/python3.9/site-packages/numpy/core/fromnumeric.py:86: FutureWarning: In a future version, DataFrame.max(axis=None) will return a scalar max over the entire DataFrame. To retain the old behavior, use 'frame.max(axis=0)' or just 'frame.max()'\n",
      "  return reduction(axis=axis, out=out, **passkwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(np.max(gene_express))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TSPAN6</th>\n",
       "      <th>TNMD</th>\n",
       "      <th>DPM1</th>\n",
       "      <th>SCYL3</th>\n",
       "      <th>C1orf112</th>\n",
       "      <th>FGR</th>\n",
       "      <th>CFH</th>\n",
       "      <th>FUCA2</th>\n",
       "      <th>GCLC</th>\n",
       "      <th>NFYA</th>\n",
       "      <th>...</th>\n",
       "      <th>PANO1</th>\n",
       "      <th>XGY2</th>\n",
       "      <th>FLJ43315</th>\n",
       "      <th>LOC105377369</th>\n",
       "      <th>PRRC2B</th>\n",
       "      <th>UGT1A3</th>\n",
       "      <th>UGT1A5</th>\n",
       "      <th>F8A2</th>\n",
       "      <th>LOC105377063</th>\n",
       "      <th>F8A1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>improve_sample_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ACH-000016</th>\n",
       "      <td>4.189825</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.126601</td>\n",
       "      <td>2.114367</td>\n",
       "      <td>2.235727</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.094553</td>\n",
       "      <td>5.907131</td>\n",
       "      <td>6.029453</td>\n",
       "      <td>3.669027</td>\n",
       "      <td>...</td>\n",
       "      <td>0.443607</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.838700</td>\n",
       "      <td>0.321928</td>\n",
       "      <td>0.163499</td>\n",
       "      <td>0.042644</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.356144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ACH-000032</th>\n",
       "      <td>0.070389</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.883376</td>\n",
       "      <td>3.549669</td>\n",
       "      <td>5.307064</td>\n",
       "      <td>0.028569</td>\n",
       "      <td>0.070389</td>\n",
       "      <td>0.286881</td>\n",
       "      <td>4.758090</td>\n",
       "      <td>5.542568</td>\n",
       "      <td>...</td>\n",
       "      <td>1.189034</td>\n",
       "      <td>0.238787</td>\n",
       "      <td>0.042644</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.320124</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.493455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ACH-000033</th>\n",
       "      <td>4.682573</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.949185</td>\n",
       "      <td>1.823749</td>\n",
       "      <td>3.837943</td>\n",
       "      <td>0.028569</td>\n",
       "      <td>0.790772</td>\n",
       "      <td>6.224002</td>\n",
       "      <td>4.825786</td>\n",
       "      <td>4.408032</td>\n",
       "      <td>...</td>\n",
       "      <td>0.963474</td>\n",
       "      <td>0.111031</td>\n",
       "      <td>0.367371</td>\n",
       "      <td>0.070389</td>\n",
       "      <td>5.638364</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.704872</td>\n",
       "      <td>3.270529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ACH-000043</th>\n",
       "      <td>3.499527</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.612647</td>\n",
       "      <td>1.952334</td>\n",
       "      <td>2.726831</td>\n",
       "      <td>0.070389</td>\n",
       "      <td>5.630813</td>\n",
       "      <td>6.098664</td>\n",
       "      <td>5.210623</td>\n",
       "      <td>3.148934</td>\n",
       "      <td>...</td>\n",
       "      <td>0.895303</td>\n",
       "      <td>0.056584</td>\n",
       "      <td>0.028569</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.616475</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.726831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ACH-000049</th>\n",
       "      <td>4.262283</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.536985</td>\n",
       "      <td>2.087463</td>\n",
       "      <td>4.051372</td>\n",
       "      <td>0.028569</td>\n",
       "      <td>2.807355</td>\n",
       "      <td>6.387328</td>\n",
       "      <td>3.507160</td>\n",
       "      <td>4.163499</td>\n",
       "      <td>...</td>\n",
       "      <td>0.367371</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.529196</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.144046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ACH-001239</th>\n",
       "      <td>4.161888</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.267349</td>\n",
       "      <td>2.572890</td>\n",
       "      <td>3.729009</td>\n",
       "      <td>0.163499</td>\n",
       "      <td>0.903038</td>\n",
       "      <td>6.166715</td>\n",
       "      <td>4.543496</td>\n",
       "      <td>3.340562</td>\n",
       "      <td>...</td>\n",
       "      <td>0.584963</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.712321</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.305241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ACH-001306</th>\n",
       "      <td>3.581351</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.933809</td>\n",
       "      <td>1.859970</td>\n",
       "      <td>3.560715</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.979111</td>\n",
       "      <td>4.281698</td>\n",
       "      <td>3.405992</td>\n",
       "      <td>4.519164</td>\n",
       "      <td>...</td>\n",
       "      <td>1.049631</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.444766</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.292782</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.650765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ACH-001307</th>\n",
       "      <td>3.988230</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.027464</td>\n",
       "      <td>1.835924</td>\n",
       "      <td>3.704872</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.485427</td>\n",
       "      <td>5.891905</td>\n",
       "      <td>3.667892</td>\n",
       "      <td>4.064366</td>\n",
       "      <td>...</td>\n",
       "      <td>0.704872</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014355</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.727648</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.426265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ACH-001318</th>\n",
       "      <td>3.983678</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.610287</td>\n",
       "      <td>2.330558</td>\n",
       "      <td>3.770829</td>\n",
       "      <td>0.056584</td>\n",
       "      <td>1.855990</td>\n",
       "      <td>7.816856</td>\n",
       "      <td>5.713421</td>\n",
       "      <td>4.729553</td>\n",
       "      <td>...</td>\n",
       "      <td>0.925999</td>\n",
       "      <td>0.084064</td>\n",
       "      <td>0.150560</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.300490</td>\n",
       "      <td>0.042644</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.084064</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.689299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ACH-001321</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.280771</td>\n",
       "      <td>3.393691</td>\n",
       "      <td>4.115200</td>\n",
       "      <td>0.028569</td>\n",
       "      <td>4.385431</td>\n",
       "      <td>5.188638</td>\n",
       "      <td>5.536675</td>\n",
       "      <td>5.106013</td>\n",
       "      <td>...</td>\n",
       "      <td>1.500802</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.150560</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.142413</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.446256</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1007 rows × 30805 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     TSPAN6  TNMD      DPM1     SCYL3  C1orf112       FGR  \\\n",
       "improve_sample_id                                                           \n",
       "ACH-000016         4.189825   0.0  6.126601  2.114367  2.235727  0.000000   \n",
       "ACH-000032         0.070389   0.0  5.883376  3.549669  5.307064  0.028569   \n",
       "ACH-000033         4.682573   0.0  6.949185  1.823749  3.837943  0.028569   \n",
       "ACH-000043         3.499527   0.0  6.612647  1.952334  2.726831  0.070389   \n",
       "ACH-000049         4.262283   0.0  6.536985  2.087463  4.051372  0.028569   \n",
       "...                     ...   ...       ...       ...       ...       ...   \n",
       "ACH-001239         4.161888   0.0  6.267349  2.572890  3.729009  0.163499   \n",
       "ACH-001306         3.581351   0.0  6.933809  1.859970  3.560715  0.000000   \n",
       "ACH-001307         3.988230   0.0  7.027464  1.835924  3.704872  0.000000   \n",
       "ACH-001318         3.983678   0.0  6.610287  2.330558  3.770829  0.056584   \n",
       "ACH-001321         1.000000   0.0  6.280771  3.393691  4.115200  0.028569   \n",
       "\n",
       "                        CFH     FUCA2      GCLC      NFYA  ...     PANO1  \\\n",
       "improve_sample_id                                          ...             \n",
       "ACH-000016         7.094553  5.907131  6.029453  3.669027  ...  0.443607   \n",
       "ACH-000032         0.070389  0.286881  4.758090  5.542568  ...  1.189034   \n",
       "ACH-000033         0.790772  6.224002  4.825786  4.408032  ...  0.963474   \n",
       "ACH-000043         5.630813  6.098664  5.210623  3.148934  ...  0.895303   \n",
       "ACH-000049         2.807355  6.387328  3.507160  4.163499  ...  0.367371   \n",
       "...                     ...       ...       ...       ...  ...       ...   \n",
       "ACH-001239         0.903038  6.166715  4.543496  3.340562  ...  0.584963   \n",
       "ACH-001306         3.979111  4.281698  3.405992  4.519164  ...  1.049631   \n",
       "ACH-001307         0.485427  5.891905  3.667892  4.064366  ...  0.704872   \n",
       "ACH-001318         1.855990  7.816856  5.713421  4.729553  ...  0.925999   \n",
       "ACH-001321         4.385431  5.188638  5.536675  5.106013  ...  1.500802   \n",
       "\n",
       "                       XGY2  FLJ43315  LOC105377369    PRRC2B    UGT1A3  \\\n",
       "improve_sample_id                                                         \n",
       "ACH-000016         0.000000  0.000000      0.000000  5.838700  0.321928   \n",
       "ACH-000032         0.238787  0.042644      0.000000  7.320124  0.000000   \n",
       "ACH-000033         0.111031  0.367371      0.070389  5.638364  0.000000   \n",
       "ACH-000043         0.056584  0.028569      0.000000  4.616475  0.000000   \n",
       "ACH-000049         0.000000  0.000000      0.000000  5.529196  0.000000   \n",
       "...                     ...       ...           ...       ...       ...   \n",
       "ACH-001239         0.000000  0.000000      0.000000  5.712321  0.000000   \n",
       "ACH-001306         0.000000  0.000000      0.000000  6.444766  0.000000   \n",
       "ACH-001307         0.000000  0.014355      0.000000  6.727648  0.000000   \n",
       "ACH-001318         0.084064  0.150560      0.000000  5.300490  0.042644   \n",
       "ACH-001321         0.000000  0.150560      0.000000  6.142413  0.000000   \n",
       "\n",
       "                     UGT1A5      F8A2  LOC105377063      F8A1  \n",
       "improve_sample_id                                              \n",
       "ACH-000016         0.163499  0.042644      0.000000  3.356144  \n",
       "ACH-000032         0.000000  0.000000      0.000000  5.493455  \n",
       "ACH-000033         0.000000  0.000000      0.704872  3.270529  \n",
       "ACH-000043         0.000000  0.000000      0.000000  3.726831  \n",
       "ACH-000049         0.000000  0.000000      0.000000  4.144046  \n",
       "...                     ...       ...           ...       ...  \n",
       "ACH-001239         0.000000  0.000000      0.000000  4.305241  \n",
       "ACH-001306         0.000000  1.292782      0.000000  4.650765  \n",
       "ACH-001307         0.000000  0.000000      0.000000  4.426265  \n",
       "ACH-001318         0.000000  1.084064      0.000000  2.689299  \n",
       "ACH-001321         0.000000  0.000000      0.000000  4.446256  \n",
       "\n",
       "[1007 rows x 30805 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gene_express"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_458042/2325377014.py:1: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  response = pd.read_csv(\"data/GDSC/response.tsv\", sep = '\\t')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>improve_sample_id</th>\n",
       "      <th>improve_chem_id</th>\n",
       "      <th>study</th>\n",
       "      <th>auc</th>\n",
       "      <th>ic50</th>\n",
       "      <th>ec50</th>\n",
       "      <th>ec50se</th>\n",
       "      <th>r2fit</th>\n",
       "      <th>einf</th>\n",
       "      <th>hs</th>\n",
       "      <th>aac1</th>\n",
       "      <th>auc1</th>\n",
       "      <th>dss1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CCLE</td>\n",
       "      <td>ACH-000956</td>\n",
       "      <td>Drug_749</td>\n",
       "      <td>fake_exp</td>\n",
       "      <td>0.7153</td>\n",
       "      <td>5.6600</td>\n",
       "      <td>5.6600</td>\n",
       "      <td>0.6867</td>\n",
       "      <td>0.9533</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.6669</td>\n",
       "      <td>0.2240</td>\n",
       "      <td>0.7760</td>\n",
       "      <td>0.1661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CCLE</td>\n",
       "      <td>ACH-000956</td>\n",
       "      <td>Drug_1326</td>\n",
       "      <td>fake_exp</td>\n",
       "      <td>0.9579</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.0230</td>\n",
       "      <td>0.7111</td>\n",
       "      <td>0.4332</td>\n",
       "      <td>0.9164</td>\n",
       "      <td>4.0000</td>\n",
       "      <td>0.0459</td>\n",
       "      <td>0.9541</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CCLE</td>\n",
       "      <td>ACH-000956</td>\n",
       "      <td>Drug_490</td>\n",
       "      <td>fake_exp</td>\n",
       "      <td>0.4130</td>\n",
       "      <td>7.5460</td>\n",
       "      <td>7.5510</td>\n",
       "      <td>0.0385</td>\n",
       "      <td>0.9948</td>\n",
       "      <td>0.0082</td>\n",
       "      <td>1.3380</td>\n",
       "      <td>0.6909</td>\n",
       "      <td>0.3091</td>\n",
       "      <td>0.6605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CCLE</td>\n",
       "      <td>ACH-000956</td>\n",
       "      <td>Drug_558</td>\n",
       "      <td>fake_exp</td>\n",
       "      <td>0.8004</td>\n",
       "      <td>5.1980</td>\n",
       "      <td>5.1980</td>\n",
       "      <td>11.7100</td>\n",
       "      <td>0.9944</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>4.0000</td>\n",
       "      <td>0.0392</td>\n",
       "      <td>0.9608</td>\n",
       "      <td>0.0291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CCLE</td>\n",
       "      <td>ACH-000956</td>\n",
       "      <td>Drug_895</td>\n",
       "      <td>fake_exp</td>\n",
       "      <td>0.5071</td>\n",
       "      <td>7.0930</td>\n",
       "      <td>7.1490</td>\n",
       "      <td>0.3175</td>\n",
       "      <td>0.8069</td>\n",
       "      <td>0.0607</td>\n",
       "      <td>1.0150</td>\n",
       "      <td>0.5470</td>\n",
       "      <td>0.4530</td>\n",
       "      <td>0.5037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>587704</th>\n",
       "      <td>GDSCv2</td>\n",
       "      <td>ACH-000475</td>\n",
       "      <td>Drug_470</td>\n",
       "      <td>19498</td>\n",
       "      <td>0.9548</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.8900</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>-0.0000</td>\n",
       "      <td>0.9096</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0452</td>\n",
       "      <td>0.9548</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>587705</th>\n",
       "      <td>GDSCv2</td>\n",
       "      <td>ACH-000475</td>\n",
       "      <td>Drug_343</td>\n",
       "      <td>19498</td>\n",
       "      <td>0.8190</td>\n",
       "      <td>3.0070</td>\n",
       "      <td>3.0070</td>\n",
       "      <td>46.7600</td>\n",
       "      <td>0.4604</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.1818</td>\n",
       "      <td>0.1943</td>\n",
       "      <td>0.8057</td>\n",
       "      <td>0.1047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>587706</th>\n",
       "      <td>GDSCv2</td>\n",
       "      <td>ACH-000475</td>\n",
       "      <td>Drug_1190</td>\n",
       "      <td>19498</td>\n",
       "      <td>0.9105</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.4040</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>-0.0000</td>\n",
       "      <td>0.8209</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0895</td>\n",
       "      <td>0.9105</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>587707</th>\n",
       "      <td>GDSCv2</td>\n",
       "      <td>ACH-000475</td>\n",
       "      <td>Drug_89</td>\n",
       "      <td>19498</td>\n",
       "      <td>0.9566</td>\n",
       "      <td>0.2428</td>\n",
       "      <td>0.2428</td>\n",
       "      <td>233.0000</td>\n",
       "      <td>0.1946</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2200</td>\n",
       "      <td>0.0438</td>\n",
       "      <td>0.9562</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>587708</th>\n",
       "      <td>GDSCv2</td>\n",
       "      <td>ACH-000475</td>\n",
       "      <td>Drug_36</td>\n",
       "      <td>19498</td>\n",
       "      <td>0.8426</td>\n",
       "      <td>4.7880</td>\n",
       "      <td>4.7880</td>\n",
       "      <td>2.8020</td>\n",
       "      <td>0.8616</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.6938</td>\n",
       "      <td>0.1111</td>\n",
       "      <td>0.8889</td>\n",
       "      <td>0.0573</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>587709 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        source improve_sample_id improve_chem_id     study     auc    ic50  \\\n",
       "0         CCLE        ACH-000956        Drug_749  fake_exp  0.7153  5.6600   \n",
       "1         CCLE        ACH-000956       Drug_1326  fake_exp  0.9579     NaN   \n",
       "2         CCLE        ACH-000956        Drug_490  fake_exp  0.4130  7.5460   \n",
       "3         CCLE        ACH-000956        Drug_558  fake_exp  0.8004  5.1980   \n",
       "4         CCLE        ACH-000956        Drug_895  fake_exp  0.5071  7.0930   \n",
       "...        ...               ...             ...       ...     ...     ...   \n",
       "587704  GDSCv2        ACH-000475        Drug_470     19498  0.9548     NaN   \n",
       "587705  GDSCv2        ACH-000475        Drug_343     19498  0.8190  3.0070   \n",
       "587706  GDSCv2        ACH-000475       Drug_1190     19498  0.9105     NaN   \n",
       "587707  GDSCv2        ACH-000475         Drug_89     19498  0.9566  0.2428   \n",
       "587708  GDSCv2        ACH-000475         Drug_36     19498  0.8426  4.7880   \n",
       "\n",
       "          ec50    ec50se   r2fit    einf      hs    aac1    auc1    dss1  \n",
       "0       5.6600    0.6867  0.9533  0.0000  0.6669  0.2240  0.7760  0.1661  \n",
       "1       7.0230    0.7111  0.4332  0.9164  4.0000  0.0459  0.9541  0.0000  \n",
       "2       7.5510    0.0385  0.9948  0.0082  1.3380  0.6909  0.3091  0.6605  \n",
       "3       5.1980   11.7100  0.9944  0.0000  4.0000  0.0392  0.9608  0.0291  \n",
       "4       7.1490    0.3175  0.8069  0.0607  1.0150  0.5470  0.4530  0.5037  \n",
       "...        ...       ...     ...     ...     ...     ...     ...     ...  \n",
       "587704  7.8900    0.0000 -0.0000  0.9096  0.0000  0.0452  0.9548  0.0000  \n",
       "587705  3.0070   46.7600  0.4604  0.0000  0.1818  0.1943  0.8057  0.1047  \n",
       "587706  3.4040    0.0000 -0.0000  0.8209  0.0000  0.0895  0.9105  0.0000  \n",
       "587707  0.2428  233.0000  0.1946  0.0000  0.2200  0.0438  0.9562  0.0000  \n",
       "587708  4.7880    2.8020  0.8616  0.0000  0.6938  0.1111  0.8889  0.0573  \n",
       "\n",
       "[587709 rows x 14 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = pd.read_csv(\"data/GDSC/response.tsv\", sep = '\\t')\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdsc_info = pd.read_csv(\"data/GDSC/GDSC_info.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ModelID</th>\n",
       "      <th>PatientID</th>\n",
       "      <th>CellLineName</th>\n",
       "      <th>StrippedCellLineName</th>\n",
       "      <th>Age</th>\n",
       "      <th>SourceType</th>\n",
       "      <th>SangerModelID</th>\n",
       "      <th>RRID</th>\n",
       "      <th>DepmapModelType</th>\n",
       "      <th>AgeCategory</th>\n",
       "      <th>...</th>\n",
       "      <th>PublicComments</th>\n",
       "      <th>WTSIMasterCellID</th>\n",
       "      <th>EngineeredModel</th>\n",
       "      <th>TreatmentStatus</th>\n",
       "      <th>OnboardedMedia</th>\n",
       "      <th>PlateCoating</th>\n",
       "      <th>OncotreeCode</th>\n",
       "      <th>OncotreeSubtype</th>\n",
       "      <th>OncotreePrimaryDisease</th>\n",
       "      <th>OncotreeLineage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>472</th>\n",
       "      <td>ACH-000475</td>\n",
       "      <td>PT-fHVhJI</td>\n",
       "      <td>huH-1</td>\n",
       "      <td>HUH1</td>\n",
       "      <td>53.0</td>\n",
       "      <td>Commercial</td>\n",
       "      <td>SIDM00586</td>\n",
       "      <td>CVCL_2956</td>\n",
       "      <td>HCC</td>\n",
       "      <td>Adult</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1855.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MF-002-001</td>\n",
       "      <td>None</td>\n",
       "      <td>HCC</td>\n",
       "      <td>Hepatocellular Carcinoma</td>\n",
       "      <td>Hepatocellular Carcinoma</td>\n",
       "      <td>Liver</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        ModelID  PatientID CellLineName StrippedCellLineName   Age  \\\n",
       "472  ACH-000475  PT-fHVhJI        huH-1                 HUH1  53.0   \n",
       "\n",
       "     SourceType SangerModelID       RRID DepmapModelType AgeCategory  ...  \\\n",
       "472  Commercial     SIDM00586  CVCL_2956             HCC       Adult  ...   \n",
       "\n",
       "    PublicComments WTSIMasterCellID EngineeredModel TreatmentStatus  \\\n",
       "472            NaN           1855.0             NaN             NaN   \n",
       "\n",
       "    OnboardedMedia PlateCoating OncotreeCode           OncotreeSubtype  \\\n",
       "472     MF-002-001         None          HCC  Hepatocellular Carcinoma   \n",
       "\n",
       "       OncotreePrimaryDisease  OncotreeLineage  \n",
       "472  Hepatocellular Carcinoma            Liver  \n",
       "\n",
       "[1 rows x 30 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gdsc_info[gdsc_info['ModelID'] == 'ACH-000475']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdsc = pd.merge(gene_express, gdsc_info, how=\"inner\", left_on=[\"improve_sample_id\"], right_on=['ModelID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdsc_x = gdsc.loc[:, gene_express.columns]\n",
    "gdsc_y = gdsc.loc[:, 'DepmapModelType']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TSPAN6</th>\n",
       "      <th>TNMD</th>\n",
       "      <th>DPM1</th>\n",
       "      <th>SCYL3</th>\n",
       "      <th>C1orf112</th>\n",
       "      <th>FGR</th>\n",
       "      <th>CFH</th>\n",
       "      <th>FUCA2</th>\n",
       "      <th>GCLC</th>\n",
       "      <th>NFYA</th>\n",
       "      <th>...</th>\n",
       "      <th>PANO1</th>\n",
       "      <th>XGY2</th>\n",
       "      <th>FLJ43315</th>\n",
       "      <th>LOC105377369</th>\n",
       "      <th>PRRC2B</th>\n",
       "      <th>UGT1A3</th>\n",
       "      <th>UGT1A5</th>\n",
       "      <th>F8A2</th>\n",
       "      <th>LOC105377063</th>\n",
       "      <th>F8A1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.189825</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.126601</td>\n",
       "      <td>2.114367</td>\n",
       "      <td>2.235727</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.094553</td>\n",
       "      <td>5.907131</td>\n",
       "      <td>6.029453</td>\n",
       "      <td>3.669027</td>\n",
       "      <td>...</td>\n",
       "      <td>0.443607</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.838700</td>\n",
       "      <td>0.321928</td>\n",
       "      <td>0.163499</td>\n",
       "      <td>0.042644</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.356144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.070389</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.883376</td>\n",
       "      <td>3.549669</td>\n",
       "      <td>5.307064</td>\n",
       "      <td>0.028569</td>\n",
       "      <td>0.070389</td>\n",
       "      <td>0.286881</td>\n",
       "      <td>4.758090</td>\n",
       "      <td>5.542568</td>\n",
       "      <td>...</td>\n",
       "      <td>1.189034</td>\n",
       "      <td>0.238787</td>\n",
       "      <td>0.042644</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.320124</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.493455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.682573</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.949185</td>\n",
       "      <td>1.823749</td>\n",
       "      <td>3.837943</td>\n",
       "      <td>0.028569</td>\n",
       "      <td>0.790772</td>\n",
       "      <td>6.224002</td>\n",
       "      <td>4.825786</td>\n",
       "      <td>4.408032</td>\n",
       "      <td>...</td>\n",
       "      <td>0.963474</td>\n",
       "      <td>0.111031</td>\n",
       "      <td>0.367371</td>\n",
       "      <td>0.070389</td>\n",
       "      <td>5.638364</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.704872</td>\n",
       "      <td>3.270529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.499527</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.612647</td>\n",
       "      <td>1.952334</td>\n",
       "      <td>2.726831</td>\n",
       "      <td>0.070389</td>\n",
       "      <td>5.630813</td>\n",
       "      <td>6.098664</td>\n",
       "      <td>5.210623</td>\n",
       "      <td>3.148934</td>\n",
       "      <td>...</td>\n",
       "      <td>0.895303</td>\n",
       "      <td>0.056584</td>\n",
       "      <td>0.028569</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.616475</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.726831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.262283</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.536985</td>\n",
       "      <td>2.087463</td>\n",
       "      <td>4.051372</td>\n",
       "      <td>0.028569</td>\n",
       "      <td>2.807355</td>\n",
       "      <td>6.387328</td>\n",
       "      <td>3.507160</td>\n",
       "      <td>4.163499</td>\n",
       "      <td>...</td>\n",
       "      <td>0.367371</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.529196</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.144046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1002</th>\n",
       "      <td>4.161888</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.267349</td>\n",
       "      <td>2.572890</td>\n",
       "      <td>3.729009</td>\n",
       "      <td>0.163499</td>\n",
       "      <td>0.903038</td>\n",
       "      <td>6.166715</td>\n",
       "      <td>4.543496</td>\n",
       "      <td>3.340562</td>\n",
       "      <td>...</td>\n",
       "      <td>0.584963</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.712321</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.305241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1003</th>\n",
       "      <td>3.581351</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.933809</td>\n",
       "      <td>1.859970</td>\n",
       "      <td>3.560715</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.979111</td>\n",
       "      <td>4.281698</td>\n",
       "      <td>3.405992</td>\n",
       "      <td>4.519164</td>\n",
       "      <td>...</td>\n",
       "      <td>1.049631</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.444766</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.292782</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.650765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1004</th>\n",
       "      <td>3.988230</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.027464</td>\n",
       "      <td>1.835924</td>\n",
       "      <td>3.704872</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.485427</td>\n",
       "      <td>5.891905</td>\n",
       "      <td>3.667892</td>\n",
       "      <td>4.064366</td>\n",
       "      <td>...</td>\n",
       "      <td>0.704872</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014355</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.727648</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.426265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1005</th>\n",
       "      <td>3.983678</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.610287</td>\n",
       "      <td>2.330558</td>\n",
       "      <td>3.770829</td>\n",
       "      <td>0.056584</td>\n",
       "      <td>1.855990</td>\n",
       "      <td>7.816856</td>\n",
       "      <td>5.713421</td>\n",
       "      <td>4.729553</td>\n",
       "      <td>...</td>\n",
       "      <td>0.925999</td>\n",
       "      <td>0.084064</td>\n",
       "      <td>0.150560</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.300490</td>\n",
       "      <td>0.042644</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.084064</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.689299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1006</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.280771</td>\n",
       "      <td>3.393691</td>\n",
       "      <td>4.115200</td>\n",
       "      <td>0.028569</td>\n",
       "      <td>4.385431</td>\n",
       "      <td>5.188638</td>\n",
       "      <td>5.536675</td>\n",
       "      <td>5.106013</td>\n",
       "      <td>...</td>\n",
       "      <td>1.500802</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.150560</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.142413</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.446256</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1007 rows × 30805 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        TSPAN6  TNMD      DPM1     SCYL3  C1orf112       FGR       CFH  \\\n",
       "0     4.189825   0.0  6.126601  2.114367  2.235727  0.000000  7.094553   \n",
       "1     0.070389   0.0  5.883376  3.549669  5.307064  0.028569  0.070389   \n",
       "2     4.682573   0.0  6.949185  1.823749  3.837943  0.028569  0.790772   \n",
       "3     3.499527   0.0  6.612647  1.952334  2.726831  0.070389  5.630813   \n",
       "4     4.262283   0.0  6.536985  2.087463  4.051372  0.028569  2.807355   \n",
       "...        ...   ...       ...       ...       ...       ...       ...   \n",
       "1002  4.161888   0.0  6.267349  2.572890  3.729009  0.163499  0.903038   \n",
       "1003  3.581351   0.0  6.933809  1.859970  3.560715  0.000000  3.979111   \n",
       "1004  3.988230   0.0  7.027464  1.835924  3.704872  0.000000  0.485427   \n",
       "1005  3.983678   0.0  6.610287  2.330558  3.770829  0.056584  1.855990   \n",
       "1006  1.000000   0.0  6.280771  3.393691  4.115200  0.028569  4.385431   \n",
       "\n",
       "         FUCA2      GCLC      NFYA  ...     PANO1      XGY2  FLJ43315  \\\n",
       "0     5.907131  6.029453  3.669027  ...  0.443607  0.000000  0.000000   \n",
       "1     0.286881  4.758090  5.542568  ...  1.189034  0.238787  0.042644   \n",
       "2     6.224002  4.825786  4.408032  ...  0.963474  0.111031  0.367371   \n",
       "3     6.098664  5.210623  3.148934  ...  0.895303  0.056584  0.028569   \n",
       "4     6.387328  3.507160  4.163499  ...  0.367371  0.000000  0.000000   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "1002  6.166715  4.543496  3.340562  ...  0.584963  0.000000  0.000000   \n",
       "1003  4.281698  3.405992  4.519164  ...  1.049631  0.000000  0.000000   \n",
       "1004  5.891905  3.667892  4.064366  ...  0.704872  0.000000  0.014355   \n",
       "1005  7.816856  5.713421  4.729553  ...  0.925999  0.084064  0.150560   \n",
       "1006  5.188638  5.536675  5.106013  ...  1.500802  0.000000  0.150560   \n",
       "\n",
       "      LOC105377369    PRRC2B    UGT1A3    UGT1A5      F8A2  LOC105377063  \\\n",
       "0         0.000000  5.838700  0.321928  0.163499  0.042644      0.000000   \n",
       "1         0.000000  7.320124  0.000000  0.000000  0.000000      0.000000   \n",
       "2         0.070389  5.638364  0.000000  0.000000  0.000000      0.704872   \n",
       "3         0.000000  4.616475  0.000000  0.000000  0.000000      0.000000   \n",
       "4         0.000000  5.529196  0.000000  0.000000  0.000000      0.000000   \n",
       "...            ...       ...       ...       ...       ...           ...   \n",
       "1002      0.000000  5.712321  0.000000  0.000000  0.000000      0.000000   \n",
       "1003      0.000000  6.444766  0.000000  0.000000  1.292782      0.000000   \n",
       "1004      0.000000  6.727648  0.000000  0.000000  0.000000      0.000000   \n",
       "1005      0.000000  5.300490  0.042644  0.000000  1.084064      0.000000   \n",
       "1006      0.000000  6.142413  0.000000  0.000000  0.000000      0.000000   \n",
       "\n",
       "          F8A1  \n",
       "0     3.356144  \n",
       "1     5.493455  \n",
       "2     3.270529  \n",
       "3     3.726831  \n",
       "4     4.144046  \n",
       "...        ...  \n",
       "1002  4.305241  \n",
       "1003  4.650765  \n",
       "1004  4.426265  \n",
       "1005  2.689299  \n",
       "1006  4.446256  \n",
       "\n",
       "[1007 rows x 30805 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gdsc_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         CCRCC\n",
       "1           BLL\n",
       "2          LUAD\n",
       "3       ZFIBSKI\n",
       "4       ZIMMEKC\n",
       "         ...   \n",
       "1002        MEL\n",
       "1003       THAP\n",
       "1004       THAP\n",
       "1005        HCC\n",
       "1006       THME\n",
       "Name: DepmapModelType, Length: 1007, dtype: object"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gdsc_y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gene ontology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of cell lines = 1225\n",
      "Total number of drugs = 684\n",
      "There are 3008 genes\n",
      "There are 1 roots: GO:0008150\n",
      "There are 2086 terms\n",
      "There are 1 connected componenets\n"
     ]
    }
   ],
   "source": [
    "training_file = \"data/drugcell_train.txt\"\n",
    "testing_file = \"data/drugcell_test.txt\"\n",
    "val_file = \"data/drugcell_val.txt\"\n",
    "cell2id_file = \"data/cell2ind.txt\"\n",
    "drug2id_file = \"data/drug2ind.txt\"\n",
    "genotype_file = \"data/cell2mutation.txt\"\n",
    "fingerprint_file = \"data/drug2fingerprint.txt\"\n",
    "onto_file = \"data/drugcell_ont.txt\"\n",
    "gene2id_file = \"data/gene2ind.txt\"\n",
    "\n",
    "train_data, feature_dict, cell2id_mapping, drug2id_mapping = prepare_train_data(training_file, \n",
    "                                                                  testing_file, cell2id_file, \n",
    "                                                                  drug2id_file)\n",
    "\n",
    "gene2id_mapping = load_mapping(gene2id_file)\n",
    "\n",
    "# load cell/drug features\n",
    "cell_features = np.genfromtxt(genotype_file, delimiter=',')\n",
    "drug_features = np.genfromtxt(fingerprint_file, delimiter=',')\n",
    "\n",
    "num_cells = len(cell2id_mapping)\n",
    "num_drugs = len(drug2id_mapping)\n",
    "num_genes = len(gene2id_mapping)\n",
    "drug_dim = len(drug_features[0,:])\n",
    "\n",
    "# load ontology\n",
    "dG, root, term_size_map, \\\n",
    "    term_direct_gene_map = load_ontology(onto_file, \n",
    "                                         gene2id_mapping)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intersect of DCell and GDSC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2983"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(set(gene2id_mapping.keys()) & set(gdsc_x.columns)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Align gene id with GDCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_intersect_list = list(set(gene2id_mapping.keys()) & set(gdsc_x.columns))\n",
    "gdsc_tensor = torch.zeros(gdsc_x.shape[0], num_genes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "for gene in gene_intersect_list:\n",
    "    idx = gene2id_mapping[gene]\n",
    "    gdsc_tensor[:,idx] = torch.tensor(gdsc_x[gene])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LUAD     74\n",
       "COAD     50\n",
       "SCLC     48\n",
       "GB       42\n",
       "PAAD     38\n",
       "         ..\n",
       "FIBS      1\n",
       "BTMOV     1\n",
       "BPLL      1\n",
       "PANET     1\n",
       "SCCO      1\n",
       "Name: DepmapModelType, Length: 138, dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gdsc_y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(gdsc_y.value_counts() >1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer_2_idx = {}\n",
    "idx_2_cancer = {}\n",
    "cancer_type_idx = []\n",
    "\n",
    "i = 0\n",
    "for cancer in gdsc_y:\n",
    "    if cancer not in cancer_2_idx:\n",
    "        cancer_2_idx[cancer] = i\n",
    "        idx_2_cancer[i] = cancer\n",
    "        cancer_type_idx.append(i)\n",
    "        \n",
    "        i += 1\n",
    "    else:\n",
    "        cancer_type_idx.append(cancer_2_idx[cancer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'CCRCC': 0,\n",
       " 'BLL': 1,\n",
       " 'LUAD': 2,\n",
       " 'ZFIBSKI': 3,\n",
       " 'ZIMMEKC': 4,\n",
       " 'RMS': 5,\n",
       " 'ZIMMLUNG': 6,\n",
       " 'MCL': 7,\n",
       " 'ZFIBBON': 8,\n",
       " 'CML': 9,\n",
       " 'MTNN': 10,\n",
       " 'ZFIBSOF': 11,\n",
       " 'AMLGATA2MECOM': 12,\n",
       " 'COAD': 13,\n",
       " 'PAAD': 14,\n",
       " 'STAD': 15,\n",
       " 'ATLL': 16,\n",
       " 'ZFIBLYM': 17,\n",
       " 'DLBCLNOS': 18,\n",
       " 'ABC': 19,\n",
       " 'AML': 20,\n",
       " 'ZIMMEPCP': 21,\n",
       " 'ATRT': 22,\n",
       " 'UCEC': 23,\n",
       " 'SCLC': 24,\n",
       " 'ESCA': 25,\n",
       " 'ZFIBBRE': 26,\n",
       " 'TSTAD': 27,\n",
       " 'ZFIBUPA': 28,\n",
       " 'BRCNOS': 29,\n",
       " 'GB': 30,\n",
       " 'MOV': 31,\n",
       " 'RCC': 32,\n",
       " 'CHS': 33,\n",
       " 'PCM': 34,\n",
       " 'ZIMMOV': 35,\n",
       " 'ZFIBLUN': 36,\n",
       " 'MRT': 37,\n",
       " 'LUSC': 38,\n",
       " 'AMKL': 39,\n",
       " 'MYCF': 40,\n",
       " 'MNG': 41,\n",
       " 'HGSOC': 42,\n",
       " 'ZIMMBR': 43,\n",
       " 'PLEMESO': 44,\n",
       " 'PAASC': 45,\n",
       " 'OCSC': 46,\n",
       " 'ESCC': 47,\n",
       " 'BLCA': 48,\n",
       " 'MEL': 49,\n",
       " 'IHCH': 50,\n",
       " 'LCLC': 51,\n",
       " 'ZFIBCOL': 52,\n",
       " 'PRAD': 53,\n",
       " 'AM': 54,\n",
       " 'AMOL': 55,\n",
       " 'BRCA': 56,\n",
       " 'NSCLC': 57,\n",
       " 'SARCNOS': 58,\n",
       " 'ES': 59,\n",
       " 'ASTR': 60,\n",
       " 'ILC': 61,\n",
       " 'PRCC': 62,\n",
       " 'EOV': 63,\n",
       " 'ALCLALKP': 64,\n",
       " 'FIBS': 65,\n",
       " 'MBL': 66,\n",
       " 'THFO': 67,\n",
       " 'BLLBCRABL1': 68,\n",
       " 'ODG': 69,\n",
       " 'HL': 70,\n",
       " 'CMLBCRABL1': 71,\n",
       " 'NBL': 72,\n",
       " 'OS': 73,\n",
       " 'ZFIBURT': 74,\n",
       " 'SOC': 75,\n",
       " 'PLBMESO': 76,\n",
       " 'IDC': 77,\n",
       " 'ARMS': 78,\n",
       " 'TLL': 79,\n",
       " 'SCCO': 80,\n",
       " 'ERMS': 81,\n",
       " 'GBAD': 82,\n",
       " 'LMS': 83,\n",
       " 'PLSMESO': 84,\n",
       " 'BL': 85,\n",
       " 'HNSC': 86,\n",
       " 'THAP': 87,\n",
       " 'PRSCC': 88,\n",
       " 'AMPCA': 89,\n",
       " 'CELNOS': 90,\n",
       " 'OPHSC': 91,\n",
       " 'HCC': 92,\n",
       " 'STSC': 93,\n",
       " 'DSTAD': 94,\n",
       " 'EHCH': 95,\n",
       " 'GSARC': 96,\n",
       " 'BTMOV': 97,\n",
       " 'READ': 98,\n",
       " 'UCS': 99,\n",
       " 'CCOV': 100,\n",
       " 'BPLL': 101,\n",
       " 'PANET': 102,\n",
       " 'DCIS': 103,\n",
       " 'STAS': 104,\n",
       " 'LUAS': 105,\n",
       " 'UCCC': 106,\n",
       " 'SKCM': 107,\n",
       " 'CLLSLL': 108,\n",
       " 'HPHSC': 109,\n",
       " 'GCLC': 110,\n",
       " 'USARC': 111,\n",
       " 'THPD': 112,\n",
       " 'MDS': 113,\n",
       " 'CESC': 114,\n",
       " 'LXSC': 115,\n",
       " 'SS': 116,\n",
       " 'DDCHS': 117,\n",
       " 'DA': 118,\n",
       " 'MBN': 119,\n",
       " 'MLADS': 120,\n",
       " 'LIHB': 121,\n",
       " 'MXOV': 122,\n",
       " 'SSRCC': 123,\n",
       " 'MSTAD': 124,\n",
       " 'LUCA': 125,\n",
       " 'MFH': 126,\n",
       " 'BLSC': 127,\n",
       " 'AASTR': 128,\n",
       " 'ESS': 129,\n",
       " 'PEL': 130,\n",
       " 'ULMS': 131,\n",
       " 'UASC': 132,\n",
       " 'UCU': 133,\n",
       " 'MACR': 134,\n",
       " 'LUMEC': 135,\n",
       " 'ECAD': 136,\n",
       " 'THME': 137}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cancer_2_idx"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dcell_vae(nn.Module):\n",
    "\n",
    "    def __init__(self, term_size_map, term_direct_gene_map, dG, ngene, root, \n",
    "                 num_hiddens_genotype, num_hiddens_final, n_class, inter_loss_penalty = 0.2):\n",
    "\n",
    "        super(dcell_vae, self).__init__()\n",
    "\n",
    "        self.root = root\n",
    "        self.num_hiddens_genotype = num_hiddens_genotype\n",
    "        self.num_hiddens_final = num_hiddens_final\n",
    "        self.n_class = n_class\n",
    "        self.inter_loss_penalty = inter_loss_penalty\n",
    "        self.dG = copy.deepcopy(dG)\n",
    "\n",
    "        # dictionary from terms to genes directly annotated with the term\n",
    "        self.term_direct_gene_map = term_direct_gene_map\n",
    "\n",
    "        self.term_visit_count = {}\n",
    "        self.init_term_visits(term_size_map)\n",
    "        \n",
    "        # calculate the number of values in a state (term): term_size_map is the number of all genes annotated with the term\n",
    "        self.term_dim_map = {}\n",
    "        self.cal_term_dim(term_size_map)\n",
    "\n",
    "        # ngenes, gene_dim are the number of all genes\n",
    "        self.gene_dim = ngene\n",
    "\n",
    "        # add modules for neural networks to process genotypes\n",
    "        self.contruct_direct_gene_layer()\n",
    "        self.construct_NN_graph(self.dG)\n",
    "\n",
    "        # add modules for final layer TODO: modify it into VAE\n",
    "        final_input_size = num_hiddens_genotype # + num_hiddens_drug[-1]\n",
    "        self.add_module('final_linear_layer', nn.Linear(final_input_size, num_hiddens_final * 2))\n",
    "        self.add_module('final_batchnorm_layer', nn.BatchNorm1d(num_hiddens_final * 2))\n",
    "        self.add_module('final_aux_linear_layer', nn.Linear(num_hiddens_final * 2, 1))\n",
    "        self.add_module('final_linear_layer_output', nn.Linear(1, 1))\n",
    "        \n",
    "        self.decoder_affine = nn.Linear(num_hiddens_final, n_class)\n",
    "\n",
    "    def init_term_visits(self, term_size_map):\n",
    "        \n",
    "        for term in term_size_map:\n",
    "            self.term_visit_count[term] = 0\n",
    "    \n",
    "    # calculate the number of values in a state (term)\n",
    "    def cal_term_dim(self, term_size_map):\n",
    "\n",
    "        for term, term_size in term_size_map.items():\n",
    "            num_output = self.num_hiddens_genotype\n",
    "\n",
    "            # log the number of hidden variables per each term\n",
    "            num_output = int(num_output)\n",
    "#            print(\"term\\t%s\\tterm_size\\t%d\\tnum_hiddens\\t%d\" % (term, term_size, num_output))\n",
    "            self.term_dim_map[term] = num_output\n",
    "\n",
    "\n",
    "    # build a layer for forwarding gene that are directly annotated with the term\n",
    "    def contruct_direct_gene_layer(self):\n",
    "\n",
    "        for term, gene_set in self.term_direct_gene_map.items():\n",
    "            if len(gene_set) == 0:\n",
    "                print('There are no directed asscoiated genes for', term)\n",
    "                sys.exit(1)\n",
    "\n",
    "            # if there are some genes directly annotated with the term, add a layer taking in all genes and forwarding out only those genes\n",
    "            self.add_module(term+'_direct_gene_layer', nn.Linear(self.gene_dim, len(gene_set)))\n",
    "\n",
    "    # start from bottom (leaves), and start building a neural network using the given ontology\n",
    "    # adding modules --- the modules are not connected yet\n",
    "    def construct_NN_graph(self, dG):\n",
    "\n",
    "        self.term_layer_list = []   # term_layer_list stores the built neural network\n",
    "        self.term_neighbor_map = {}\n",
    "\n",
    "        # term_neighbor_map records all children of each term\n",
    "        for term in dG.nodes():\n",
    "            self.term_neighbor_map[term] = []\n",
    "            for child in dG.neighbors(term):\n",
    "                self.term_neighbor_map[term].append(child)\n",
    "\n",
    "        while True:\n",
    "            leaves = [n for n in dG.nodes() if dG.out_degree(n) == 0]\n",
    "            #leaves = [n for n,d in dG.out_degree().items() if d==0]\n",
    "            #leaves = [n for n,d in dG.out_degree() if d==0]\n",
    "\n",
    "            if len(leaves) == 0:\n",
    "                break\n",
    "\n",
    "            self.term_layer_list.append(leaves)\n",
    "\n",
    "            for term in leaves:\n",
    "\n",
    "                # input size will be #chilren + #genes directly annotated by the term\n",
    "                input_size = 0\n",
    "\n",
    "                for child in self.term_neighbor_map[term]:\n",
    "                    input_size += self.term_dim_map[child]\n",
    "\n",
    "                if term in self.term_direct_gene_map:\n",
    "                    input_size += len(self.term_direct_gene_map[term])\n",
    "\n",
    "                # term_hidden is the number of the hidden variables in each state\n",
    "                term_hidden = self.term_dim_map[term]\n",
    "\n",
    "                self.add_module(term+'_linear_layer', nn.Linear(input_size, term_hidden))\n",
    "                self.add_module(term+'_batchnorm_layer', nn.BatchNorm1d(term_hidden))\n",
    "                self.add_module(term+'_aux_linear_layer1', nn.Linear(term_hidden, self.n_class))\n",
    "                self.add_module(term+'_aux_linear_layer2', nn.Linear(self.n_class, self.n_class))\n",
    "\n",
    "            dG.remove_nodes_from(leaves)\n",
    "\n",
    "\n",
    "    # definition of encoder\n",
    "    def encoder(self, x):\n",
    "        gene_input = x.narrow(1, 0, self.gene_dim)\n",
    "\n",
    "        # define forward function for genotype dcell #############################################\n",
    "        term_gene_out_map = {}\n",
    "\n",
    "        for term, _ in self.term_direct_gene_map.items():\n",
    "            term_gene_out_map[term] = self._modules[term + '_direct_gene_layer'](gene_input)\n",
    "\n",
    "        term_NN_out_map = {}\n",
    "        aux_out_map = {}\n",
    "\n",
    "        for i, layer in enumerate(self.term_layer_list):\n",
    "\n",
    "            for term in layer:\n",
    "\n",
    "                child_input_list = []\n",
    "\n",
    "                self.term_visit_count[term] += 1\n",
    "                \n",
    "                for child in self.term_neighbor_map[term]:\n",
    "                    child_input_list.append(term_NN_out_map[child])\n",
    "\n",
    "                if term in self.term_direct_gene_map:\n",
    "                    child_input_list.append(term_gene_out_map[term])\n",
    "\n",
    "                child_input = torch.cat(child_input_list,1)\n",
    "\n",
    "                term_NN_out = self._modules[term+'_linear_layer'](child_input)\n",
    "\n",
    "                Tanh_out = torch.tanh(term_NN_out)\n",
    "                term_NN_out_map[term] = self._modules[term+'_batchnorm_layer'](Tanh_out)\n",
    "                aux_layer1_out = torch.tanh(self._modules[term+'_aux_linear_layer1'](term_NN_out_map[term]))\n",
    "                aux_out_map[term] = self._modules[term+'_aux_linear_layer2'](aux_layer1_out)\n",
    "\n",
    "        # connect two neural networks at the top #################################################\n",
    "        final_input = term_NN_out_map[self.root] # torch.cat((term_NN_out_map[self.root], drug_out), 1)\n",
    "\n",
    "        out = self._modules['final_batchnorm_layer'](torch.tanh(self._modules['final_linear_layer'](final_input)))\n",
    "        term_NN_out_map['final'] = out\n",
    "\n",
    "        aux_layer_out = torch.tanh(self._modules['final_aux_linear_layer'](out))\n",
    "        aux_out_map['final'] = self._modules['final_linear_layer_output'](aux_layer_out)\n",
    "\n",
    "        return aux_out_map, term_NN_out_map\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        aux_out_map, term_NN_out_map = self.encoder(x)\n",
    "        \n",
    "        mu = term_NN_out_map['final'][..., :self.num_hiddens_final]\n",
    "        log_var = term_NN_out_map['final'][..., :self.num_hiddens_final]  # T X batch X z_dim\n",
    "        std_dec = log_var.mul(0.5).exp_()\n",
    "        # std_dec = 1\n",
    "        \n",
    "        latent = MultivariateNormal(loc = mu, \n",
    "                                    scale_tril=torch.diag_embed(std_dec))\n",
    "        z = latent.rsample()\n",
    "        \n",
    "        recon_mean = self.decoder_affine(z)\n",
    "        logits = F.softmax(recon_mean, -1)\n",
    "\n",
    "        return logits, mu, log_var, aux_out_map, term_NN_out_map\n",
    "    \n",
    "    def loss_log_vae(self, logits, y, mu, log_var, beta = 0.001):\n",
    "        # y: true labels\n",
    "        ori_y_shape = y.shape\n",
    "        \n",
    "        class_loss = F.cross_entropy(logits.view(-1, logits.shape[-1]), \n",
    "                                     y.reshape(-1), reduction = 'none').div(np.log(2)).view(*ori_y_shape)\n",
    "        \n",
    "        KLD = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp(), \n",
    "                              dim = -1)\n",
    "        \n",
    "        log_loss = class_loss + beta * KLD\n",
    "        log_loss = torch.mean(torch.logsumexp(log_loss, 0))\n",
    "        \n",
    "        return log_loss\n",
    "    \n",
    "    def intermediate_loss(self, aux_out_map, y):\n",
    "        \n",
    "        inter_loss = 0\n",
    "        for name, output in aux_out_map.items():\n",
    "            if name == 'final':\n",
    "                inter_loss += 0\n",
    "            else: # change 0.2 to smaller one for big terms\n",
    "                ori_y_shape = y.shape\n",
    "        \n",
    "                term_loss = F.cross_entropy(output, \n",
    "                                             y, \n",
    "                                             reduction = 'none').div(np.log(2)).view(*ori_y_shape)\n",
    "                inter_loss += term_loss\n",
    "\n",
    "        return inter_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNASeqData(Dataset):\n",
    "    \n",
    "    def __init__(self, X, c=None, y=None, transform=None):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.c = c\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        sample = self.X[index,:]\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            sample = self.transform(sample)\n",
    "        \n",
    "        if self.y is not None and self.c is None:\n",
    "            return sample, self.y[index]\n",
    "        elif self.y is not None and self.c is not None:\n",
    "            return sample, self.y[index], self.c[index]\n",
    "        elif self.y is None and self.c is not None:\n",
    "            return sample, self.c[index]\n",
    "        else:\n",
    "            return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "y = torch.tensor(cancer_type_idx)\n",
    "gdsc_dataset = RNASeqData(X = gdsc_tensor, y = y)\n",
    "training_set, testing_set = random_split(gdsc_dataset, [0.8, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_epochs = 200\n",
    "\n",
    "train_loader = DataLoader(training_set, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(testing_set, batch_size=len(testing_set), shuffle=False)\n",
    "(inputdata, response) = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "201"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(testing_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_term_mask(term_direct_gene_map, gene_dim, device):\n",
    "\n",
    "    term_mask_map = {}\n",
    "\n",
    "    for term, gene_set in term_direct_gene_map.items():\n",
    "\n",
    "        mask = torch.zeros(len(gene_set), gene_dim)\n",
    "\n",
    "        for i, gene_id in enumerate(gene_set):\n",
    "            mask[i, gene_id] = 1\n",
    "\n",
    "        mask_gpu = torch.autograd.Variable(mask)\n",
    "\n",
    "        term_mask_map[term] = mask_gpu.to(device)\n",
    "\n",
    "    return term_mask_map\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 17/100 [17:05<1:20:16, 58.04s/it, Epoch=16, Loss=2.36e+3, Accuracy=0.0498]"
     ]
    }
   ],
   "source": [
    "num_hiddens_genotype = 6\n",
    "num_hiddens_final = 6\n",
    "\n",
    "model = dcell_vae(term_size_map, term_direct_gene_map, dG, num_genes, \n",
    "                 root, num_hiddens_genotype, num_hiddens_final, n_class = len(cancer_2_idx))\n",
    "model.to(DEVICE)\n",
    "term_mask_map = create_term_mask(model.term_direct_gene_map, num_genes, device = DEVICE)\n",
    "\n",
    "\n",
    "learning_rate = 0.001\n",
    "torch.manual_seed(0)\n",
    "training_loss_list = []\n",
    "accu_list = []\n",
    "train_epochs = 100\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, betas=(0.9, 0.99), eps=1e-05)\n",
    "term_mask_map = create_term_mask(model.term_direct_gene_map, gene_dim=num_genes, device=DEVICE)\n",
    "\n",
    "optimizer.zero_grad()\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    term_name = name.split('_')[0]\n",
    "\n",
    "    if '_direct_gene_layer.weight' in name:\n",
    "        param.data = torch.mul(param.data, term_mask_map[term_name].to(DEVICE)) * 0.1\n",
    "    else:\n",
    "        param.data = param.data * 0.1\n",
    "\n",
    "tepoch = tqdm.tqdm(range(train_epochs))\n",
    "for epoch in tepoch:\n",
    "    # Train\n",
    "    model.train()\n",
    "    train_predict = torch.zeros(0, 0).to(DEVICE)\n",
    "\n",
    "    for i, (data, response) in enumerate(train_loader):\n",
    "        # Convert torch tensor to Variable\n",
    "\n",
    "        # Forward + Backward + Optimize\n",
    "        optimizer.zero_grad()  # zero the gradient buffer\n",
    "\n",
    "        # Here term_NN_out_map is a dictionary\n",
    "        recon_mean, mu, log_var, aux_out_map, term_NN_out_map = model(data.to(DEVICE))\n",
    "\n",
    "        if train_predict.size()[0] == 0:\n",
    "            train_predict = aux_out_map[\"final\"].data\n",
    "        else:\n",
    "            train_predict = torch.cat([train_predict, aux_out_map[\"final\"].data], dim=0)\n",
    "\n",
    "        total_loss = 0\n",
    "\n",
    "        loss_vae = model.loss_log_vae(\n",
    "            logits=recon_mean, y=response.to(DEVICE), mu=mu, log_var=log_var, beta=0.001\n",
    "        )\n",
    "\n",
    "        loss_intermidiate = model.intermediate_loss(aux_out_map, response.to(DEVICE))\n",
    "\n",
    "        total_loss = torch.mean(loss_vae + model.inter_loss_penalty * loss_intermidiate)\n",
    "        \n",
    "        tmp_loss = total_loss.item()\n",
    "        \n",
    "        total_loss.backward()\n",
    "\n",
    "        for name, param in model.named_parameters():\n",
    "            if \"_direct_gene_layer.weight\" not in name:\n",
    "                continue\n",
    "            term_name = name.split(\"_\")[0]\n",
    "            # print name, param.grad.data.size(), term_mask_map[term_name].size()\n",
    "            param.grad.data = torch.mul(param.grad.data, term_mask_map[term_name])\n",
    "\n",
    "        optimizer.step()\n",
    "    \n",
    "    (inputdata, response) = next(iter(test_loader))\n",
    "    recon_mean, mu, log_var, aux_out_map, term_NN_out_map = model(inputdata.to(DEVICE))\n",
    "\n",
    "    accu = torch.sum(torch.argmax(recon_mean, 1).cpu() == response)/len(response)\n",
    "    \n",
    "    tepoch.set_postfix({\"Epoch\": epoch, \"Loss\": tmp_loss, \"Accuracy\": accu.item()})\n",
    "    \n",
    "    training_loss_list.append(tmp_loss)\n",
    "    accu_list.append(accu.item())\n",
    "    # if epoch % 10 == 0:\n",
    "    torch.save(model, \"gdsc_50.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(\"gdsc_50.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top 5 test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_top5 = copy.deepcopy(cancer_type_idx)\n",
    "\n",
    "for idx, y_tmp in enumerate(cancer_type_idx):\n",
    "    if y_tmp in [2,13,24,30,14]:\n",
    "        y_top5[idx] = 1\n",
    "    else:\n",
    "        y_top5[idx] = 0\n",
    "    \n",
    "y_top5_train = y_top5[:700]\n",
    "y_top5_test = y_top5[700:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "y = torch.tensor(cancer_type_idx)\n",
    "gdsc_dataset = RNASeqData(X = gdsc_tensor, y = torch.tensor(y_top5))\n",
    "training_set, testing_set = random_split(gdsc_dataset, [0.8, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_epochs = 256\n",
    "\n",
    "train_loader = DataLoader(training_set, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(testing_set, batch_size=len(testing_set), shuffle=False)\n",
    "# (inputdata, response) = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_term_mask(term_direct_gene_map, gene_dim, device):\n",
    "\n",
    "    term_mask_map = {}\n",
    "\n",
    "    for term, gene_set in term_direct_gene_map.items():\n",
    "\n",
    "        mask = torch.zeros(len(gene_set), gene_dim)\n",
    "\n",
    "        for i, gene_id in enumerate(gene_set):\n",
    "            mask[i, gene_id] = 1\n",
    "\n",
    "        mask_gpu = torch.autograd.Variable(mask)\n",
    "\n",
    "        term_mask_map[term] = mask_gpu.to(device)\n",
    "\n",
    "    return term_mask_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [33:54<00:00, 40.69s/it, Epoch=49, Loss=298, Accuracy=0.791, AUC=0.784]\n",
      "100%|██████████| 50/50 [34:01<00:00, 40.83s/it, Epoch=49, Loss=302, Accuracy=0.796, AUC=0.806]\n",
      " 38%|███▊      | 19/50 [13:01<20:23, 39.46s/it, Epoch=18, Loss=298, Accuracy=0.806, AUC=0.789]"
     ]
    }
   ],
   "source": [
    "for seed in range(10):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    training_set, testing_set = random_split(gdsc_dataset, [0.8, 0.2])\n",
    "    train_loader = DataLoader(training_set, batch_size=64, shuffle=True)\n",
    "    test_loader = DataLoader(testing_set, batch_size=len(testing_set), shuffle=False)\n",
    "\n",
    "    num_hiddens_genotype = 6\n",
    "    num_hiddens_final = 6\n",
    "\n",
    "    model = dcell_vae(term_size_map, term_direct_gene_map, dG, num_genes, \n",
    "                    root, num_hiddens_genotype, num_hiddens_final, n_class = 2)\n",
    "    model.to(DEVICE)\n",
    "    term_mask_map = create_term_mask(model.term_direct_gene_map, num_genes, device = DEVICE)\n",
    "\n",
    "\n",
    "    learning_rate = 0.001\n",
    "    # torch.manual_seed(0)\n",
    "    training_loss_list = []\n",
    "    accu_list = []\n",
    "    auc_list = []\n",
    "    train_epochs = 50\n",
    "    best_auc = 0\n",
    "\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, betas=(0.9, 0.99), eps=1e-05)\n",
    "    term_mask_map = create_term_mask(model.term_direct_gene_map, gene_dim=num_genes, device=DEVICE)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        term_name = name.split('_')[0]\n",
    "\n",
    "        if '_direct_gene_layer.weight' in name:\n",
    "            param.data = torch.mul(param.data, term_mask_map[term_name].to(DEVICE)) * 0.1\n",
    "        else:\n",
    "            param.data = param.data * 0.1\n",
    "\n",
    "    tepoch = tqdm.tqdm(range(train_epochs))\n",
    "    for epoch in tepoch:\n",
    "        # Train\n",
    "        model.train()\n",
    "        train_predict = torch.zeros(0, 0).to(DEVICE)\n",
    "        (inputdata_test, response_test) = next(iter(test_loader))\n",
    "        \n",
    "        for i, (data, response) in enumerate(train_loader):\n",
    "            # Convert torch tensor to Variable\n",
    "\n",
    "            # Forward + Backward + Optimize\n",
    "            optimizer.zero_grad()  # zero the gradient buffer\n",
    "\n",
    "            # Here term_NN_out_map is a dictionary\n",
    "            recon_mean, mu, log_var, aux_out_map, term_NN_out_map = model(data.to(DEVICE))\n",
    "\n",
    "            if train_predict.size()[0] == 0:\n",
    "                train_predict = aux_out_map[\"final\"].data\n",
    "            else:\n",
    "                train_predict = torch.cat([train_predict, aux_out_map[\"final\"].data], dim=0)\n",
    "\n",
    "            total_loss = 0\n",
    "\n",
    "            loss_vae = model.loss_log_vae(\n",
    "                logits=recon_mean, y=response.to(DEVICE), mu=mu, log_var=log_var, beta=0.001\n",
    "            )\n",
    "\n",
    "            loss_intermidiate = model.intermediate_loss(aux_out_map, response.to(DEVICE))\n",
    "\n",
    "            total_loss = torch.mean(loss_vae + model.inter_loss_penalty * loss_intermidiate)\n",
    "            \n",
    "            tmp_loss = total_loss.item()\n",
    "            \n",
    "            total_loss.backward()\n",
    "\n",
    "            for name, param in model.named_parameters():\n",
    "                if \"_direct_gene_layer.weight\" not in name:\n",
    "                    continue\n",
    "                term_name = name.split(\"_\")[0]\n",
    "                # print name, param.grad.data.size(), term_mask_map[term_name].size()\n",
    "                param.grad.data = torch.mul(param.grad.data, term_mask_map[term_name])\n",
    "\n",
    "            optimizer.step()\n",
    "        \n",
    "        model.eval()\n",
    "        recon_mean, mu, log_var, aux_out_map, term_NN_out_map = model(inputdata_test.to(DEVICE))\n",
    "\n",
    "        accu = torch.sum(torch.argmax(recon_mean, 1).detach().cpu() == response_test)/len(response_test)\n",
    "        \n",
    "        auc = roc_auc_score(response_test, recon_mean[:,1].detach().cpu())\n",
    "        \n",
    "        tepoch.set_postfix({\"Epoch\": epoch, \"Loss\": tmp_loss, \"Accuracy\": accu.item(), 'AUC': auc})\n",
    "        \n",
    "        training_loss_list.append(tmp_loss)\n",
    "        accu_list.append(accu.item())\n",
    "        auc_list.append(auc)\n",
    "        # if epoch % 10 == 0:\n",
    "        if auc > best_auc:\n",
    "            torch.save(model, \"gdsc_50_top5_data_\"+str(seed)+\".pt\")\n",
    "            best_auc = auc\n",
    "    with open('loss_accu_top5_data_'+str(seed)+'.pkl', 'wb') as f:\n",
    "        pickle.dump({'loss': training_loss_list,\n",
    "                    'accu': accu_list,\n",
    "                    'auc': auc_list,\n",
    "                    'best_auc': best_auc}, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "\n",
    "with open('loss_accu_top5_new.pkl', 'wb') as f:\n",
    "    pickle.dump({'loss': training_loss_list,\n",
    "                'accu': accu_list,\n",
    "                'auc': auc_list}, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_hiddens_genotype = 6\n",
    "# num_hiddens_final = 6\n",
    "\n",
    "# model = dcell_vae(term_size_map, term_direct_gene_map, dG, num_genes, \n",
    "#                     root, num_hiddens_genotype, num_hiddens_final, n_class = 2)\n",
    "model = torch.load(\"gdsc_50_top5_\"+str(9)+\".pt\").to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([201, 3008])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(inputdata, response) = next(iter(test_loader))\n",
    "inputdata.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:10<00:00,  1.82it/s]\n",
      "100%|██████████| 20/20 [00:11<00:00,  1.79it/s]\n",
      "100%|██████████| 20/20 [00:10<00:00,  1.83it/s]\n",
      "100%|██████████| 20/20 [00:11<00:00,  1.74it/s]\n",
      "100%|██████████| 20/20 [00:10<00:00,  1.83it/s]\n",
      "100%|██████████| 20/20 [00:11<00:00,  1.81it/s]\n",
      "100%|██████████| 20/20 [00:10<00:00,  1.83it/s]\n",
      "100%|██████████| 20/20 [00:11<00:00,  1.80it/s]\n",
      "100%|██████████| 20/20 [00:10<00:00,  1.85it/s]\n",
      "100%|██████████| 20/20 [00:11<00:00,  1.81it/s]\n"
     ]
    }
   ],
   "source": [
    "auc_avg_list = []\n",
    "for model_idx in range(10):\n",
    "    model = torch.load(\"gdsc_50_top5_\"+str(model_idx)+\".pt\").to(DEVICE)\n",
    "    \n",
    "    recon_mean_avg = 0\n",
    "    for j in tqdm.tqdm(range(20)):\n",
    "        recon_mean, mu, log_var, aux_out_map, term_NN_out_map = model(inputdata.to(DEVICE))\n",
    "        recon_mean_avg += recon_mean.cpu().detach()\n",
    "        \n",
    "    recon_mean_avg /= 20\n",
    "    auc_avg = roc_auc_score(response, recon_mean_avg[:,1])\n",
    "    auc_avg_list.append(auc_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8388829215896885,\n",
       " 0.8842642320085928,\n",
       " 0.8427765843179378,\n",
       " 0.8782223415682062,\n",
       " 0.8917830290010741,\n",
       " 0.8646616541353385,\n",
       " 0.8707035445757251,\n",
       " 0.8654672395273899,\n",
       " 0.8425080558539204,\n",
       " 0.8441192266380236]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auc_avg_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8259)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recon_mean, mu, log_var, aux_out_map, term_NN_out_map = model(inputdata.to(DEVICE))\n",
    "\n",
    "\n",
    "torch.sum(torch.argmax(recon_mean, 1).cpu() == response)/len(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r\"loss_accu_top5.pkl\", \"rb\") as input_file:\n",
    "   res = pickle.load(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    with open(r\"loss_accu_top5\"+str(i)+\".pkl\", \"rb\") as input_file:\n",
    "        res = pickle.load(input_file)\n",
    "        print(res['best_auc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding drug embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Drugcell_Vae(nn.Module):\n",
    "\n",
    "    def __init__(self, term_size_map, term_direct_gene_map, dG, ngene, ndrug, root, \n",
    "                 num_hiddens_genotype, num_hiddens_drug, num_hiddens_final, \n",
    "                 n_class, inter_loss_penalty = 0.2):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.root = root\n",
    "        self.num_hiddens_genotype = num_hiddens_genotype\n",
    "        self.num_hiddens_drug = num_hiddens_drug\n",
    "        \n",
    "        \n",
    "        self.num_hiddens_final = num_hiddens_final\n",
    "        self.n_class = n_class\n",
    "        self.inter_loss_penalty = inter_loss_penalty\n",
    "        self.dG = copy.deepcopy(dG)\n",
    "\n",
    "        # dictionary from terms to genes directly annotated with the term\n",
    "        self.term_direct_gene_map = term_direct_gene_map\n",
    "\n",
    "        self.term_visit_count = {}\n",
    "        self.init_term_visits(term_size_map)\n",
    "        \n",
    "        # calculate the number of values in a state (term): term_size_map is the number of all genes annotated with the term\n",
    "        self.term_dim_map = {}\n",
    "        self.cal_term_dim(term_size_map)\n",
    "\n",
    "        # ngenes, gene_dim are the number of all genes\n",
    "        self.gene_dim = ngene\n",
    "        self.drug_dim = ndrug\n",
    "\n",
    "        # add modules for neural networks to process genotypes\n",
    "        self.contruct_direct_gene_layer()\n",
    "        self.construct_NN_graph(self.dG)\n",
    "\n",
    "        # add modules for neural networks to process drugs\n",
    "        self.construct_NN_drug()\n",
    "\n",
    "        # add modules for final layer TODO: modify it into VAE\n",
    "        final_input_size = num_hiddens_genotype + num_hiddens_drug[-1]\n",
    "        self.add_module('final_linear_layer', nn.Linear(final_input_size, num_hiddens_final * 2))\n",
    "        self.add_module('final_batchnorm_layer', nn.BatchNorm1d(num_hiddens_final * 2))\n",
    "        self.add_module('final_aux_linear_layer', nn.Linear(num_hiddens_final * 2, 1))\n",
    "        self.add_module('final_linear_layer_output', nn.Linear(1, 1))\n",
    "        \n",
    "        self.decoder_affine = nn.Linear(num_hiddens_final, 1)\n",
    "\n",
    "    def init_term_visits(self, term_size_map):\n",
    "        \n",
    "        for term in term_size_map:\n",
    "            self.term_visit_count[term] = 0\n",
    "    \n",
    "    # calculate the number of values in a state (term)\n",
    "    def cal_term_dim(self, term_size_map):\n",
    "\n",
    "        for term, term_size in term_size_map.items():\n",
    "            num_output = self.num_hiddens_genotype\n",
    "\n",
    "            # log the number of hidden variables per each term\n",
    "            num_output = int(num_output)\n",
    "#            print(\"term\\t%s\\tterm_size\\t%d\\tnum_hiddens\\t%d\" % (term, term_size, num_output))\n",
    "            self.term_dim_map[term] = num_output\n",
    "\n",
    "\n",
    "    # build a layer for forwarding gene that are directly annotated with the term\n",
    "    def contruct_direct_gene_layer(self):\n",
    "\n",
    "        for term, gene_set in self.term_direct_gene_map.items():\n",
    "            if len(gene_set) == 0:\n",
    "                print('There are no directed asscoiated genes for', term)\n",
    "                sys.exit(1)\n",
    "\n",
    "            # if there are some genes directly annotated with the term, add a layer taking in all genes and forwarding out only those genes\n",
    "            self.add_module(term+'_direct_gene_layer', nn.Linear(self.gene_dim, len(gene_set)))\n",
    "\n",
    "\n",
    "    # add modules for fully connected neural networks for drug processing\n",
    "    def construct_NN_drug(self):\n",
    "        input_size = self.drug_dim\n",
    "\n",
    "        for i in range(len(self.num_hiddens_drug)):\n",
    "            self.add_module('drug_linear_layer_' + str(i+1), nn.Linear(input_size, self.num_hiddens_drug[i]))\n",
    "            self.add_module('drug_batchnorm_layer_' + str(i+1), nn.BatchNorm1d(self.num_hiddens_drug[i]))\n",
    "            self.add_module('drug_aux_linear_layer1_' + str(i+1), nn.Linear(self.num_hiddens_drug[i],1))\n",
    "            self.add_module('drug_aux_linear_layer2_' + str(i+1), nn.Linear(1,1))\n",
    "\n",
    "            input_size = self.num_hiddens_drug[i]\n",
    "\n",
    "    # start from bottom (leaves), and start building a neural network using the given ontology\n",
    "    # adding modules --- the modules are not connected yet\n",
    "    def construct_NN_graph(self, dG):\n",
    "\n",
    "        self.term_layer_list = []   # term_layer_list stores the built neural network\n",
    "        self.term_neighbor_map = {}\n",
    "\n",
    "        # term_neighbor_map records all children of each term\n",
    "        for term in dG.nodes():\n",
    "            self.term_neighbor_map[term] = []\n",
    "            for child in dG.neighbors(term):\n",
    "                self.term_neighbor_map[term].append(child)\n",
    "\n",
    "        while True:\n",
    "            leaves = [n for n in dG.nodes() if dG.out_degree(n) == 0]\n",
    "            #leaves = [n for n,d in dG.out_degree().items() if d==0]\n",
    "            #leaves = [n for n,d in dG.out_degree() if d==0]\n",
    "\n",
    "            if len(leaves) == 0:\n",
    "                break\n",
    "\n",
    "            self.term_layer_list.append(leaves)\n",
    "\n",
    "            for term in leaves:\n",
    "\n",
    "                # input size will be #chilren + #genes directly annotated by the term\n",
    "                input_size = 0\n",
    "\n",
    "                for child in self.term_neighbor_map[term]:\n",
    "                    input_size += self.term_dim_map[child]\n",
    "\n",
    "                if term in self.term_direct_gene_map:\n",
    "                    input_size += len(self.term_direct_gene_map[term])\n",
    "\n",
    "                # term_hidden is the number of the hidden variables in each state\n",
    "                term_hidden = self.term_dim_map[term]\n",
    "\n",
    "                self.add_module(term+'_linear_layer', nn.Linear(input_size, term_hidden))\n",
    "                self.add_module(term+'_batchnorm_layer', nn.BatchNorm1d(term_hidden))\n",
    "                self.add_module(term+'_aux_linear_layer1', nn.Linear(term_hidden, term_hidden))\n",
    "                self.add_module(term+'_aux_linear_layer2', nn.Linear(term_hidden, 1))\n",
    "\n",
    "            dG.remove_nodes_from(leaves)\n",
    "\n",
    "\n",
    "    # definition of encoder\n",
    "    def encoder(self, x):\n",
    "        gene_input = x.narrow(1, 0, self.gene_dim)\n",
    "        drug_input = x.narrow(1, self.gene_dim, self.drug_dim)\n",
    "        \n",
    "        # define forward function for genotype dcell #############################################\n",
    "        term_gene_out_map = {}\n",
    "\n",
    "        for term, _ in self.term_direct_gene_map.items():\n",
    "            term_gene_out_map[term] = self._modules[term + '_direct_gene_layer'](gene_input)\n",
    "\n",
    "        term_NN_out_map = {}\n",
    "        aux_out_map = {}\n",
    "\n",
    "        for i, layer in enumerate(self.term_layer_list):\n",
    "\n",
    "            for term in layer:\n",
    "\n",
    "                child_input_list = []\n",
    "\n",
    "                self.term_visit_count[term] += 1\n",
    "                \n",
    "                for child in self.term_neighbor_map[term]:\n",
    "                    child_input_list.append(term_NN_out_map[child])\n",
    "\n",
    "                if term in self.term_direct_gene_map:\n",
    "                    child_input_list.append(term_gene_out_map[term])\n",
    "\n",
    "                child_input = torch.cat(child_input_list,1)\n",
    "\n",
    "                term_NN_out = self._modules[term+'_linear_layer'](child_input)\n",
    "\n",
    "                Tanh_out = torch.tanh(term_NN_out)\n",
    "                term_NN_out_map[term] = self._modules[term+'_batchnorm_layer'](Tanh_out)\n",
    "                aux_layer1_out = torch.tanh(self._modules[term+'_aux_linear_layer1'](term_NN_out_map[term]))\n",
    "                aux_out_map[term] = self._modules[term+'_aux_linear_layer2'](aux_layer1_out)\n",
    "\n",
    "        drug_out = drug_input\n",
    "\n",
    "        for i in range(1, len(self.num_hiddens_drug)+1, 1):\n",
    "            drug_out = self._modules['drug_batchnorm_layer_'+str(i)]( torch.tanh(self._modules['drug_linear_layer_' + str(i)](drug_out)))\n",
    "            term_NN_out_map['drug_'+str(i)] = drug_out\n",
    "\n",
    "            aux_layer1_out = torch.tanh(self._modules['drug_aux_linear_layer1_'+str(i)](drug_out))\n",
    "            aux_out_map['drug_'+str(i)] = self._modules['drug_aux_linear_layer2_'+str(i)](aux_layer1_out)\n",
    "\n",
    "\n",
    "        # connect two neural networks at the top #################################################\n",
    "        final_input = torch.cat((term_NN_out_map[self.root], drug_out), 1)\n",
    "\n",
    "        out = self._modules['final_batchnorm_layer'](torch.tanh(self._modules['final_linear_layer'](final_input)))\n",
    "        term_NN_out_map['final'] = out\n",
    "\n",
    "        aux_layer_out = torch.tanh(self._modules['final_aux_linear_layer'](out))\n",
    "        aux_out_map['final'] = self._modules['final_linear_layer_output'](aux_layer_out)\n",
    "\n",
    "        return aux_out_map, term_NN_out_map\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        aux_out_map, term_NN_out_map = self.encoder(x)\n",
    "        \n",
    "        mu = term_NN_out_map['final'][..., :self.num_hiddens_final]\n",
    "        log_var = term_NN_out_map['final'][..., :self.num_hiddens_final]  # T X batch X z_dim\n",
    "        std_dec = log_var.mul(0.5).exp_()\n",
    "        # std_dec = 1\n",
    "        \n",
    "        latent = MultivariateNormal(loc = mu, \n",
    "                                    scale_tril=torch.diag_embed(std_dec))\n",
    "        z = latent.rsample()\n",
    "        \n",
    "        recon_mean = self.decoder_affine(z)\n",
    "        recon_mean = F.sigmoid(recon_mean)\n",
    "\n",
    "        return recon_mean, mu, log_var, aux_out_map, term_NN_out_map\n",
    "    \n",
    "    def loss_log_vae(self, recon_mean, y, mu, log_var, beta = 0.001):\n",
    "        # y: true labels\n",
    "        ori_y_shape = y.shape\n",
    "        \n",
    "        class_loss = F.mse_loss(recon_mean.view(-1), \n",
    "                                     y.reshape(-1), reduction = 'none').div(np.log(2)).view(*ori_y_shape)\n",
    "        \n",
    "        KLD = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp(), \n",
    "                              dim = -1)\n",
    "        \n",
    "        log_loss = class_loss + beta * KLD\n",
    "        log_loss = torch.mean(torch.logsumexp(log_loss, 0))\n",
    "        \n",
    "        return log_loss\n",
    "    \n",
    "    def intermediate_loss(self, aux_out_map, y):\n",
    "        \n",
    "        inter_loss = 0\n",
    "        for name, output in aux_out_map.items():\n",
    "            if name == 'final':\n",
    "                inter_loss += 0\n",
    "            else: # change 0.2 to smaller one for big terms\n",
    "                ori_y_shape = y.shape\n",
    "        \n",
    "                term_loss = F.mse_loss(output.view(-1), \n",
    "                                             y.reshape(-1), \n",
    "                                             reduction = 'none').div(np.log(2)).view(*ori_y_shape)\n",
    "                inter_loss += term_loss\n",
    "\n",
    "        return inter_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feature, train_label, test_feature, test_label = train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000, 2])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_feature.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_feature' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m du\u001b[38;5;241m.\u001b[39mDataLoader(du\u001b[38;5;241m.\u001b[39mTensorDataset(\u001b[43mtrain_feature\u001b[49m,train_label),\n\u001b[1;32m      2\u001b[0m                                  batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_feature' is not defined"
     ]
    }
   ],
   "source": [
    "train_loader = du.DataLoader(du.TensorDataset(train_feature,train_label),\n",
    "                                 batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_features = np.genfromtxt(genotype_file, delimiter=',')\n",
    "drug_features = np.genfromtxt(fingerprint_file, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ecfp4.0</th>\n",
       "      <th>ecfp4.1</th>\n",
       "      <th>ecfp4.2</th>\n",
       "      <th>ecfp4.3</th>\n",
       "      <th>ecfp4.4</th>\n",
       "      <th>ecfp4.5</th>\n",
       "      <th>ecfp4.6</th>\n",
       "      <th>ecfp4.7</th>\n",
       "      <th>ecfp4.8</th>\n",
       "      <th>ecfp4.9</th>\n",
       "      <th>...</th>\n",
       "      <th>ecfp4.502</th>\n",
       "      <th>ecfp4.503</th>\n",
       "      <th>ecfp4.504</th>\n",
       "      <th>ecfp4.505</th>\n",
       "      <th>ecfp4.506</th>\n",
       "      <th>ecfp4.507</th>\n",
       "      <th>ecfp4.508</th>\n",
       "      <th>ecfp4.509</th>\n",
       "      <th>ecfp4.510</th>\n",
       "      <th>ecfp4.511</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>improve_chem_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Drug_0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Drug_1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Drug_10</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Drug_100</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Drug_1000</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Drug_995</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Drug_996</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Drug_997</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Drug_998</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Drug_999</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1565 rows × 512 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 ecfp4.0  ecfp4.1  ecfp4.2  ecfp4.3  ecfp4.4  ecfp4.5  \\\n",
       "improve_chem_id                                                         \n",
       "Drug_0                 0        0        0        0        0        0   \n",
       "Drug_1                 0        0        0        0        0        0   \n",
       "Drug_10                0        0        0        0        0        0   \n",
       "Drug_100               0        0        0        0        0        0   \n",
       "Drug_1000              0        1        0        0        0        0   \n",
       "...                  ...      ...      ...      ...      ...      ...   \n",
       "Drug_995               0        1        0        0        0        0   \n",
       "Drug_996               0        1        0        0        0        0   \n",
       "Drug_997               0        1        0        0        0        0   \n",
       "Drug_998               0        1        0        0        0        0   \n",
       "Drug_999               0        1        0        0        0        0   \n",
       "\n",
       "                 ecfp4.6  ecfp4.7  ecfp4.8  ecfp4.9  ...  ecfp4.502  \\\n",
       "improve_chem_id                                      ...              \n",
       "Drug_0                 0        0        0        0  ...          0   \n",
       "Drug_1                 0        0        0        0  ...          0   \n",
       "Drug_10                0        0        0        0  ...          0   \n",
       "Drug_100               0        0        0        0  ...          0   \n",
       "Drug_1000              0        0        0        0  ...          0   \n",
       "...                  ...      ...      ...      ...  ...        ...   \n",
       "Drug_995               0        0        0        0  ...          0   \n",
       "Drug_996               0        0        0        0  ...          0   \n",
       "Drug_997               0        0        0        0  ...          0   \n",
       "Drug_998               0        0        0        0  ...          0   \n",
       "Drug_999               0        0        0        0  ...          0   \n",
       "\n",
       "                 ecfp4.503  ecfp4.504  ecfp4.505  ecfp4.506  ecfp4.507  \\\n",
       "improve_chem_id                                                          \n",
       "Drug_0                   0          0          0          0          0   \n",
       "Drug_1                   0          0          0          0          0   \n",
       "Drug_10                  0          0          0          0          0   \n",
       "Drug_100                 0          1          0          0          0   \n",
       "Drug_1000                0          0          0          1          0   \n",
       "...                    ...        ...        ...        ...        ...   \n",
       "Drug_995                 0          0          0          0          1   \n",
       "Drug_996                 0          0          1          0          0   \n",
       "Drug_997                 1          0          0          0          0   \n",
       "Drug_998                 0          0          0          0          0   \n",
       "Drug_999                 0          0          0          0          0   \n",
       "\n",
       "                 ecfp4.508  ecfp4.509  ecfp4.510  ecfp4.511  \n",
       "improve_chem_id                                              \n",
       "Drug_0                   0          0          0          0  \n",
       "Drug_1                   0          0          0          0  \n",
       "Drug_10                  0          0          0          0  \n",
       "Drug_100                 0          0          0          0  \n",
       "Drug_1000                0          0          0          0  \n",
       "...                    ...        ...        ...        ...  \n",
       "Drug_995                 0          1          0          0  \n",
       "Drug_996                 0          0          0          0  \n",
       "Drug_997                 0          0          0          0  \n",
       "Drug_998                 0          0          0          0  \n",
       "Drug_999                 0          0          0          0  \n",
       "\n",
       "[1565 rows x 512 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drug_ecfp4_nbits512 = pd.read_csv(\"data/GDSC/drug_ecfp4_nbits512.tsv\", sep = '\\t', index_col=0)\n",
    "drug_ecfp4_nbits512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3008\n",
      "2048\n",
      "64\n"
     ]
    }
   ],
   "source": [
    "genedim = len(cell_features[0,:])\n",
    "drugdim = len(drug_features[0,:])\n",
    "print(genedim)\n",
    "print(drugdim)\n",
    "feature = np.zeros((inputdata.size()[0], (genedim+drugdim)))\n",
    "#print(input_data)\n",
    "print(inputdata.size()[0])\n",
    "#print(drug_features)\n",
    "\n",
    "for i in range(inputdata.size()[0]):\n",
    "    #print(int(input_data[i,0]))\n",
    "    try:\n",
    "        feature[i] = np.concatenate((cell_features[int(inputdata[i,0])], \n",
    "                                        drug_features[int(inputdata[i,1])]), axis=None)\n",
    "    except IndexError:\n",
    "        pass\n",
    "\n",
    "feature = torch.from_numpy(feature).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "genedim = len(cell_features[0,:])\n",
    "drugdim = len(drug_features[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_458042/2325377014.py:1: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  response = pd.read_csv(\"data/GDSC/response.tsv\", sep = '\\t')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>improve_sample_id</th>\n",
       "      <th>improve_chem_id</th>\n",
       "      <th>study</th>\n",
       "      <th>auc</th>\n",
       "      <th>ic50</th>\n",
       "      <th>ec50</th>\n",
       "      <th>ec50se</th>\n",
       "      <th>r2fit</th>\n",
       "      <th>einf</th>\n",
       "      <th>hs</th>\n",
       "      <th>aac1</th>\n",
       "      <th>auc1</th>\n",
       "      <th>dss1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CCLE</td>\n",
       "      <td>ACH-000956</td>\n",
       "      <td>Drug_749</td>\n",
       "      <td>fake_exp</td>\n",
       "      <td>0.7153</td>\n",
       "      <td>5.6600</td>\n",
       "      <td>5.6600</td>\n",
       "      <td>0.6867</td>\n",
       "      <td>0.9533</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.6669</td>\n",
       "      <td>0.2240</td>\n",
       "      <td>0.7760</td>\n",
       "      <td>0.1661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CCLE</td>\n",
       "      <td>ACH-000956</td>\n",
       "      <td>Drug_1326</td>\n",
       "      <td>fake_exp</td>\n",
       "      <td>0.9579</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.0230</td>\n",
       "      <td>0.7111</td>\n",
       "      <td>0.4332</td>\n",
       "      <td>0.9164</td>\n",
       "      <td>4.0000</td>\n",
       "      <td>0.0459</td>\n",
       "      <td>0.9541</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CCLE</td>\n",
       "      <td>ACH-000956</td>\n",
       "      <td>Drug_490</td>\n",
       "      <td>fake_exp</td>\n",
       "      <td>0.4130</td>\n",
       "      <td>7.5460</td>\n",
       "      <td>7.5510</td>\n",
       "      <td>0.0385</td>\n",
       "      <td>0.9948</td>\n",
       "      <td>0.0082</td>\n",
       "      <td>1.3380</td>\n",
       "      <td>0.6909</td>\n",
       "      <td>0.3091</td>\n",
       "      <td>0.6605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CCLE</td>\n",
       "      <td>ACH-000956</td>\n",
       "      <td>Drug_558</td>\n",
       "      <td>fake_exp</td>\n",
       "      <td>0.8004</td>\n",
       "      <td>5.1980</td>\n",
       "      <td>5.1980</td>\n",
       "      <td>11.7100</td>\n",
       "      <td>0.9944</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>4.0000</td>\n",
       "      <td>0.0392</td>\n",
       "      <td>0.9608</td>\n",
       "      <td>0.0291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CCLE</td>\n",
       "      <td>ACH-000956</td>\n",
       "      <td>Drug_895</td>\n",
       "      <td>fake_exp</td>\n",
       "      <td>0.5071</td>\n",
       "      <td>7.0930</td>\n",
       "      <td>7.1490</td>\n",
       "      <td>0.3175</td>\n",
       "      <td>0.8069</td>\n",
       "      <td>0.0607</td>\n",
       "      <td>1.0150</td>\n",
       "      <td>0.5470</td>\n",
       "      <td>0.4530</td>\n",
       "      <td>0.5037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>587704</th>\n",
       "      <td>GDSCv2</td>\n",
       "      <td>ACH-000475</td>\n",
       "      <td>Drug_470</td>\n",
       "      <td>19498</td>\n",
       "      <td>0.9548</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.8900</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>-0.0000</td>\n",
       "      <td>0.9096</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0452</td>\n",
       "      <td>0.9548</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>587705</th>\n",
       "      <td>GDSCv2</td>\n",
       "      <td>ACH-000475</td>\n",
       "      <td>Drug_343</td>\n",
       "      <td>19498</td>\n",
       "      <td>0.8190</td>\n",
       "      <td>3.0070</td>\n",
       "      <td>3.0070</td>\n",
       "      <td>46.7600</td>\n",
       "      <td>0.4604</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.1818</td>\n",
       "      <td>0.1943</td>\n",
       "      <td>0.8057</td>\n",
       "      <td>0.1047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>587706</th>\n",
       "      <td>GDSCv2</td>\n",
       "      <td>ACH-000475</td>\n",
       "      <td>Drug_1190</td>\n",
       "      <td>19498</td>\n",
       "      <td>0.9105</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.4040</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>-0.0000</td>\n",
       "      <td>0.8209</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0895</td>\n",
       "      <td>0.9105</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>587707</th>\n",
       "      <td>GDSCv2</td>\n",
       "      <td>ACH-000475</td>\n",
       "      <td>Drug_89</td>\n",
       "      <td>19498</td>\n",
       "      <td>0.9566</td>\n",
       "      <td>0.2428</td>\n",
       "      <td>0.2428</td>\n",
       "      <td>233.0000</td>\n",
       "      <td>0.1946</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2200</td>\n",
       "      <td>0.0438</td>\n",
       "      <td>0.9562</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>587708</th>\n",
       "      <td>GDSCv2</td>\n",
       "      <td>ACH-000475</td>\n",
       "      <td>Drug_36</td>\n",
       "      <td>19498</td>\n",
       "      <td>0.8426</td>\n",
       "      <td>4.7880</td>\n",
       "      <td>4.7880</td>\n",
       "      <td>2.8020</td>\n",
       "      <td>0.8616</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.6938</td>\n",
       "      <td>0.1111</td>\n",
       "      <td>0.8889</td>\n",
       "      <td>0.0573</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>587709 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        source improve_sample_id improve_chem_id     study     auc    ic50  \\\n",
       "0         CCLE        ACH-000956        Drug_749  fake_exp  0.7153  5.6600   \n",
       "1         CCLE        ACH-000956       Drug_1326  fake_exp  0.9579     NaN   \n",
       "2         CCLE        ACH-000956        Drug_490  fake_exp  0.4130  7.5460   \n",
       "3         CCLE        ACH-000956        Drug_558  fake_exp  0.8004  5.1980   \n",
       "4         CCLE        ACH-000956        Drug_895  fake_exp  0.5071  7.0930   \n",
       "...        ...               ...             ...       ...     ...     ...   \n",
       "587704  GDSCv2        ACH-000475        Drug_470     19498  0.9548     NaN   \n",
       "587705  GDSCv2        ACH-000475        Drug_343     19498  0.8190  3.0070   \n",
       "587706  GDSCv2        ACH-000475       Drug_1190     19498  0.9105     NaN   \n",
       "587707  GDSCv2        ACH-000475         Drug_89     19498  0.9566  0.2428   \n",
       "587708  GDSCv2        ACH-000475         Drug_36     19498  0.8426  4.7880   \n",
       "\n",
       "          ec50    ec50se   r2fit    einf      hs    aac1    auc1    dss1  \n",
       "0       5.6600    0.6867  0.9533  0.0000  0.6669  0.2240  0.7760  0.1661  \n",
       "1       7.0230    0.7111  0.4332  0.9164  4.0000  0.0459  0.9541  0.0000  \n",
       "2       7.5510    0.0385  0.9948  0.0082  1.3380  0.6909  0.3091  0.6605  \n",
       "3       5.1980   11.7100  0.9944  0.0000  4.0000  0.0392  0.9608  0.0291  \n",
       "4       7.1490    0.3175  0.8069  0.0607  1.0150  0.5470  0.4530  0.5037  \n",
       "...        ...       ...     ...     ...     ...     ...     ...     ...  \n",
       "587704  7.8900    0.0000 -0.0000  0.9096  0.0000  0.0452  0.9548  0.0000  \n",
       "587705  3.0070   46.7600  0.4604  0.0000  0.1818  0.1943  0.8057  0.1047  \n",
       "587706  3.4040    0.0000 -0.0000  0.8209  0.0000  0.0895  0.9105  0.0000  \n",
       "587707  0.2428  233.0000  0.1946  0.0000  0.2200  0.0438  0.9562  0.0000  \n",
       "587708  4.7880    2.8020  0.8616  0.0000  0.6938  0.1111  0.8889  0.0573  \n",
       "\n",
       "[587709 rows x 14 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = pd.read_csv(\"data/GDSC/response.tsv\", sep = '\\t')\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "improve_sample_key_id_dict = {k: v for v, k in enumerate(response['improve_sample_id'].unique())}\n",
    "improve_sample_id_key_dict = {v: k for v, k in enumerate(response['improve_sample_id'].unique())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "chem_key_id_dict = {k: v for v, k in enumerate(response['improve_sample_id'].unique())}\n",
    "chem_id_key_dict = {v: k for v, k in enumerate(response['improve_sample_id'].unique())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 1]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drug_ecfp4_nbits512.loc[['Drug_749', 'Drug_36'],:].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_intersect_list = list(set(gene2id_mapping.keys()) & set(gdsc_x.columns))\n",
    "gdsc_tensor = torch.zeros(gene_express.shape[0], num_genes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "for gene in gene_intersect_list:\n",
    "    idx = gene2id_mapping[gene]\n",
    "    gdsc_tensor[:,idx] = torch.tensor(gene_express[gene])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_intersect_list = list(set(gene2id_mapping.keys()) & set(gdsc_x.columns))\n",
    "# gdsc_tensor = torch.zeros(gene_express.shape[0], num_genes)\n",
    "gdsc_row_key_id = {k: v for v, k in enumerate(gene_express.index)}\n",
    "gdsc_row_id_key = {v: k for v, k in enumerate(gene_express.index)}\n",
    "\n",
    "chem_row_key_id = {k: v for v, k in enumerate(drug_ecfp4_nbits512.index)}\n",
    "chem_row_id_key = {v: k for v, k in enumerate(drug_ecfp4_nbits512.index)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "drug_tensor = torch.tensor(drug_ecfp4_nbits512.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_gdcs2 = response[response['source'] == 'GDSCv2'].loc[:,['improve_sample_id', \n",
    "                                                             'improve_chem_id',\n",
    "                                                             'auc']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_gdcs2 = response_gdcs2.replace({'improve_sample_id': gdsc_row_key_id})\n",
    "response_gdcs2 = response_gdcs2.replace({'improve_chem_id': chem_row_key_id})\n",
    "response_gdcs2 = torch.tensor(response_gdcs2.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[9.4900e+02, 6.7400e+02, 5.8330e-01],\n",
       "        [9.4900e+02, 6.7400e+02, 6.0260e-01],\n",
       "        [9.4900e+02, 6.7400e+02, 4.3030e-01],\n",
       "        ...,\n",
       "        [5.1500e+02, 2.1400e+02, 9.1050e-01],\n",
       "        [5.1500e+02, 1.4430e+03, 9.5660e-01],\n",
       "        [5.1500e+02, 8.5500e+02, 8.4260e-01]], dtype=torch.float64)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_gdcs2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "586"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.unique(list(zip(response_gdcs2[:,:2]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = response_gdcs2[:,:2].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(114644, 2)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_unique = np.unique(a, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(66353, 2)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_unique.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GDSCData(Dataset):\n",
    "    \n",
    "    def __init__(self, response, gene_tensor, chem_tensor):\n",
    "        self.response = response\n",
    "        self.gene_tensor = gene_tensor\n",
    "        self.chem_tensor = chem_tensor\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.response.shape[0]\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        sample = self.response[index,:]\n",
    "        \n",
    "        X_gene = self.gene_tensor[sample[0].long() ,:]\n",
    "        X_chem = self.chem_tensor[sample[1].long() ,:]\n",
    "        \n",
    "        y = sample[2]\n",
    "\n",
    "        X = torch.cat((X_gene, X_chem), 0)\n",
    "        \n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gdcs_idx = torch.unique(response_gdcs2[:,0], sorted=False)[:423]\n",
    "test_gdcs_idx = torch.unique(response_gdcs2[:,0], sorted=False)[423:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdsc_data = GDSCData(response_gdcs2, gdsc_tensor, drug_tensor)\n",
    "gdsc_data_train = GDSCData(response_gdcs2[torch.isin(response_gdcs2[:,0], train_gdcs_idx)].float(), gdsc_tensor, drug_tensor)\n",
    "gdsc_data_test = GDSCData(response_gdcs2[torch.isin(response_gdcs2[:,0], test_gdcs_idx)].float(), gdsc_tensor, drug_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_GDSC_np = response_gdcs2[torch.isin(response_gdcs2[:,0], train_gdcs_idx)].float().numpy()\n",
    "test_GDSC_np = response_gdcs2[torch.isin(response_gdcs2[:,0], test_gdcs_idx)].float().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(59389, 2)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(train_GDSC_np[:,:2], axis=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6964, 2)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(test_GDSC_np[:,:2], axis=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(gdsc_data_train, batch_size=8192, shuffle=True)\n",
    "test_loader = DataLoader(gdsc_data_test, batch_size=8192, shuffle=False)\n",
    "(inputdata, response) = next(iter(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "(inputdata, response) = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0909, 2.1043, 2.8480,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        [0.1243, 4.4555, 1.7655,  ..., 0.0000, 1.0000, 0.0000],\n",
       "        [0.3448, 3.8227, 3.8866,  ..., 0.0000, 1.0000, 0.0000],\n",
       "        ...,\n",
       "        [0.3334, 3.8063, 3.3827,  ..., 0.0000, 0.0000, 1.0000],\n",
       "        [1.0909, 3.0807, 1.9411,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        [0.0566, 1.9373, 2.5236,  ..., 0.0000, 0.0000, 0.0000]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19271"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(gdsc_data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "187it [10:12,  3.27s/it, Epoch=0, Training Loss=17.8, Testing Loss=0.0617]\n",
      "187it [10:08,  3.25s/it, Epoch=1, Training Loss=23.1, Testing Loss=0.0301]\n",
      "187it [10:08,  3.26s/it, Epoch=2, Training Loss=18.1, Testing Loss=0.022] \n",
      "187it [10:32,  3.38s/it, Epoch=3, Training Loss=18, Testing Loss=0.0191]  \n",
      "187it [10:41,  3.43s/it, Epoch=4, Training Loss=18.7, Testing Loss=0.0171]\n",
      "187it [10:12,  3.27s/it, Epoch=5, Training Loss=18.9, Testing Loss=0.0159]\n",
      "187it [10:09,  3.26s/it, Epoch=6, Training Loss=16.5, Testing Loss=0.0149]\n",
      "187it [10:10,  3.26s/it, Epoch=7, Training Loss=23.8, Testing Loss=0.0153]\n",
      "187it [10:08,  3.25s/it, Epoch=8, Training Loss=18.2, Testing Loss=0.0144]\n",
      "187it [10:09,  3.26s/it, Epoch=9, Training Loss=18, Testing Loss=0.015]   \n",
      "187it [10:07,  3.25s/it, Epoch=10, Training Loss=22.2, Testing Loss=0.0142]\n",
      "187it [10:08,  3.25s/it, Epoch=11, Training Loss=20.1, Testing Loss=0.014] \n",
      "187it [10:10,  3.26s/it, Epoch=12, Training Loss=17.3, Testing Loss=0.0147]\n",
      "187it [10:14,  3.28s/it, Epoch=13, Training Loss=16.3, Testing Loss=0.0138]\n",
      "187it [10:16,  3.30s/it, Epoch=14, Training Loss=18.4, Testing Loss=0.0139]\n",
      "187it [10:10,  3.26s/it, Epoch=15, Training Loss=16.1, Testing Loss=0.0139]\n",
      "187it [10:09,  3.26s/it, Epoch=16, Training Loss=16.3, Testing Loss=0.0136]\n",
      "187it [10:07,  3.25s/it, Epoch=17, Training Loss=17.3, Testing Loss=0.0138]\n",
      "187it [10:09,  3.26s/it, Epoch=18, Training Loss=18.9, Testing Loss=0.0134]\n",
      "187it [10:10,  3.26s/it, Epoch=19, Training Loss=18.8, Testing Loss=0.0134]\n",
      "187it [10:10,  3.26s/it, Epoch=20, Training Loss=17.4, Testing Loss=0.0135]\n",
      "187it [10:08,  3.26s/it, Epoch=21, Training Loss=20.6, Testing Loss=0.013] \n",
      "187it [10:06,  3.24s/it, Epoch=22, Training Loss=16.9, Testing Loss=0.0133]\n",
      "187it [10:09,  3.26s/it, Epoch=23, Training Loss=15.3, Testing Loss=0.0133]\n",
      "187it [10:09,  3.26s/it, Epoch=24, Training Loss=18.1, Testing Loss=0.0134]\n",
      "187it [10:16,  3.30s/it, Epoch=25, Training Loss=18.1, Testing Loss=0.0134]\n",
      "187it [10:10,  3.27s/it, Epoch=26, Training Loss=19.9, Testing Loss=0.0125]\n",
      "187it [10:14,  3.28s/it, Epoch=27, Training Loss=19.4, Testing Loss=0.0131]\n",
      "187it [10:09,  3.26s/it, Epoch=28, Training Loss=17.7, Testing Loss=0.0129]\n",
      "187it [10:11,  3.27s/it, Epoch=29, Training Loss=17.4, Testing Loss=0.0132]\n",
      "187it [10:12,  3.27s/it, Epoch=30, Training Loss=18.7, Testing Loss=0.0128]\n",
      "187it [10:09,  3.26s/it, Epoch=31, Training Loss=18.2, Testing Loss=0.0131]\n",
      "187it [10:10,  3.26s/it, Epoch=32, Training Loss=13.6, Testing Loss=0.013] \n",
      "187it [10:12,  3.28s/it, Epoch=33, Training Loss=16.4, Testing Loss=0.013] \n",
      "187it [10:11,  3.27s/it, Epoch=34, Training Loss=18.7, Testing Loss=0.0127]\n",
      "187it [10:09,  3.26s/it, Epoch=35, Training Loss=19.6, Testing Loss=0.0129]\n",
      "187it [10:09,  3.26s/it, Epoch=36, Training Loss=18.5, Testing Loss=0.0128]\n",
      "187it [10:09,  3.26s/it, Epoch=37, Training Loss=21.5, Testing Loss=0.0136]\n",
      "187it [10:08,  3.25s/it, Epoch=38, Training Loss=18.8, Testing Loss=0.013] \n",
      "187it [10:11,  3.27s/it, Epoch=39, Training Loss=19, Testing Loss=0.0126]  \n",
      "187it [10:12,  3.27s/it, Epoch=40, Training Loss=16.1, Testing Loss=0.0126]\n",
      "187it [10:12,  3.27s/it, Epoch=41, Training Loss=22.4, Testing Loss=0.0132]\n",
      "187it [10:14,  3.28s/it, Epoch=42, Training Loss=17.9, Testing Loss=0.0128]\n",
      "187it [10:13,  3.28s/it, Epoch=43, Training Loss=19.7, Testing Loss=0.013] \n",
      "187it [10:11,  3.27s/it, Epoch=44, Training Loss=23.1, Testing Loss=0.0131]\n",
      "187it [10:05,  3.24s/it, Epoch=45, Training Loss=19.7, Testing Loss=0.0127]\n",
      "187it [10:06,  3.24s/it, Epoch=46, Training Loss=16.2, Testing Loss=0.0126]\n",
      "187it [10:08,  3.25s/it, Epoch=47, Training Loss=18.8, Testing Loss=0.0129]\n",
      "187it [10:09,  3.26s/it, Epoch=48, Training Loss=17.7, Testing Loss=0.0128]\n",
      "187it [10:07,  3.25s/it, Epoch=49, Training Loss=18.8, Testing Loss=0.0125]\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1)\n",
    "\n",
    "num_hiddens_genotype = 6\n",
    "num_hiddens_final = 6\n",
    "drug_hiddens='100,50,6'\n",
    "num_hiddens_drug = list(map(int, drug_hiddens.split(',')))\n",
    "# num_hiddens_drug = 6\n",
    "num_drugs = drug_ecfp4_nbits512.shape[1]\n",
    "\n",
    "model = Drugcell_Vae(term_size_map, term_direct_gene_map, dG, num_genes, num_drugs, \n",
    "                 root, num_hiddens_genotype, num_hiddens_drug, num_hiddens_final, \n",
    "                 n_class = len(cancer_2_idx))\n",
    "model.to(DEVICE)\n",
    "term_mask_map = create_term_mask(model.term_direct_gene_map, num_genes, device = DEVICE)\n",
    "\n",
    "\n",
    "learning_rate = 0.001\n",
    "torch.manual_seed(0)\n",
    "training_loss_list = []\n",
    "testing_loss_list = []\n",
    "epoch_list = []\n",
    "accu_list = []\n",
    "train_epochs = 50\n",
    "best_loss = 1000\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, betas=(0.9, 0.99), eps=1e-05)\n",
    "term_mask_map = create_term_mask(model.term_direct_gene_map, gene_dim=num_genes, device=DEVICE)\n",
    "\n",
    "optimizer.zero_grad()\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    term_name = name.split('_')[0]\n",
    "\n",
    "    if '_direct_gene_layer.weight' in name:\n",
    "        param.data = torch.mul(param.data, term_mask_map[term_name].to(DEVICE)) * 0.1\n",
    "    else:\n",
    "        param.data = param.data * 0.1\n",
    "\n",
    "mse_tmp_testing = torch.tensor(0, device=DEVICE)\n",
    "# tepoch = tqdm.tqdm(range(train_epochs))\n",
    "for epoch in range(train_epochs):\n",
    "    # Train\n",
    "    model.train()\n",
    "    train_predict = torch.zeros(0, 0).to(DEVICE)\n",
    "\n",
    "    tloader = tqdm.tqdm(enumerate(train_loader))\n",
    "    for i, (data, response) in tloader:\n",
    "        # Convert torch tensor to Variable\n",
    "\n",
    "        # Forward + Backward + Optimize\n",
    "        optimizer.zero_grad()  # zero the gradient buffer\n",
    "\n",
    "        # Here term_NN_out_map is a dictionary\n",
    "        recon_mean, mu, log_var, aux_out_map, term_NN_out_map = model(data.to(DEVICE))\n",
    "\n",
    "        if train_predict.size()[0] == 0:\n",
    "            train_predict = aux_out_map[\"final\"].data\n",
    "        else:\n",
    "            train_predict = torch.cat([train_predict, aux_out_map[\"final\"].data], dim=0)\n",
    "\n",
    "        total_loss = 0\n",
    "\n",
    "        loss_vae = model.loss_log_vae(\n",
    "            recon_mean=recon_mean, y=response.to(DEVICE), mu=mu, log_var=log_var, beta=0.001\n",
    "        )\n",
    "\n",
    "        loss_intermidiate = model.intermediate_loss(aux_out_map, response.to(DEVICE))\n",
    "\n",
    "        total_loss = torch.mean(loss_vae + model.inter_loss_penalty * loss_intermidiate)\n",
    "        \n",
    "        tmp_loss = total_loss.item()\n",
    "        # tepoch.set_postfix({\"Epoch\": epoch, \n",
    "        #                     \"Training Loss\": tmp_loss, \n",
    "        #                     \"Testing Loss\": mse_tmp_testing.item()})\n",
    "        \n",
    "        total_loss.backward()\n",
    "\n",
    "        for name, param in model.named_parameters():\n",
    "            if \"_direct_gene_layer.weight\" not in name:\n",
    "                continue\n",
    "            term_name = name.split(\"_\")[0]\n",
    "            # print name, param.grad.data.size(), term_mask_map[term_name].size()\n",
    "            param.grad.data = torch.mul(param.grad.data, term_mask_map[term_name])\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % 6 == 0:\n",
    "            with torch.no_grad():\n",
    "                (inputdata, response) = next(iter(test_loader))\n",
    "                recon_mean, mu, log_var, aux_out_map, term_NN_out_map = model(inputdata.to(DEVICE))\n",
    "\n",
    "                mse_tmp_testing = F.mse_loss(recon_mean.detach().squeeze().cpu(), response.squeeze())\n",
    "\n",
    "                tloader.set_postfix({\"Epoch\": epoch, \n",
    "                                    \"Training Loss\": tmp_loss, \n",
    "                                    \"Testing Loss\": mse_tmp_testing.item()})\n",
    "                \n",
    "                training_loss_list.append(tmp_loss)\n",
    "                testing_loss_list.append(mse_tmp_testing.item())\n",
    "                epoch_list.append(epoch)\n",
    "    with torch.no_grad():\n",
    "        (inputdata, response) = next(iter(test_loader))\n",
    "        recon_mean, mu, log_var, aux_out_map, term_NN_out_map = model(inputdata.to(DEVICE))\n",
    "\n",
    "        mse_tmp_testing = F.mse_loss(recon_mean.detach().squeeze().cpu(), response.squeeze())\n",
    "        \n",
    "        if mse_tmp_testing < best_loss:\n",
    "            torch.save(model, \"gdsc_drug_epoch_new.pt\")\n",
    "    # if epoch % 10 == 0:\n",
    "    # torch.save(model, \"gdsc_50.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0%100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "model= torch.load(\"gdsc_drug_epoch_new.pt\", map_location=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'recon_mean' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[73], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m mse_tmp_testing \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmse_loss(\u001b[43mrecon_mean\u001b[49m\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39msqueeze()\u001b[38;5;241m.\u001b[39mcpu(), response\u001b[38;5;241m.\u001b[39msqueeze()\u001b[38;5;241m.\u001b[39mcpu())\n",
      "\u001b[0;31mNameError\u001b[0m: name 'recon_mean' is not defined"
     ]
    }
   ],
   "source": [
    "mse_tmp_testing = F.mse_loss(recon_mean.detach().squeeze().cpu(), response.squeeze().cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1446)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sqrt(mse_tmp_testing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12it [01:28,  7.38s/it]04:02<?, ?it/s, Epoch=0, Training Loss=424, Testing Loss=0.11] \n",
    "12it [01:25,  7.16s/it]05:34<?, ?it/s, Epoch=1, Training Loss=409, Testing Loss=0.107]\n",
    "12it [01:28,  7.38s/it]07:11<?, ?it/s, Epoch=2, Training Loss=393, Testing Loss=0.105]\n",
    "12it [01:28,  7.37s/it]08:42<?, ?it/s, Epoch=3, Training Loss=372, Testing Loss=0.102]\n",
    "12it [01:34,  7.87s/it]10:30<?, ?it/s, Epoch=4, Training Loss=346, Testing Loss=0.0985]\n",
    "12it [01:26,  7.19s/it]12:04<?, ?it/s, Epoch=5, Training Loss=314, Testing Loss=0.0943]\n",
    "12it [01:26,  7.22s/it]13:35<?, ?it/s, Epoch=6, Training Loss=278, Testing Loss=0.0913]\n",
    "12it [01:26,  7.24s/it]15:10<?, ?it/s, Epoch=7, Training Loss=236, Testing Loss=0.0872]\n",
    "12it [01:22,  6.89s/it]16:41<?, ?it/s, Epoch=8, Training Loss=191, Testing Loss=0.0838]\n",
    "12it [01:26,  7.24s/it]18:19<?, ?it/s, Epoch=9, Training Loss=148, Testing Loss=0.0789]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0088)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse_tmp_testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model, \"gdsc_drug_12_epoch.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ori_y_shape = response.shape\n",
    "F.mse_loss(recon_mean.view(-1), \n",
    "                                     response.to(DEVICE).reshape(-1), reduction = 'none').div(np.log(2)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(-0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp(), \n",
    "                              dim = -1)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/homes/ac.tfeng/miniconda3/envs/general/lib/python3.9/site-packages/torch/nn/functional.py:1967: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    (inputdata, response) = next(iter(test_loader))\n",
    "    recon_mean, mu, log_var, aux_out_map, term_NN_out_map = model(inputdata.to(DEVICE))\n",
    "\n",
    "    mse_tmp_testing = F.mse_loss(recon_mean.detach().squeeze().cpu(), response.squeeze())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0126)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse_tmp_testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1121)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sqrt(mse_tmp_testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0925)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "torch.sqrt(mse_tmp_testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.6719],\n",
       "        [0.6408],\n",
       "        [0.6014],\n",
       "        ...,\n",
       "        [0.8601],\n",
       "        [0.7687],\n",
       "        [0.8636]], device='cuda:6')"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recon_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5833, 0.6026, 0.4303,  ..., 0.9992, 0.9854, 1.0000])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Predict')"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(recon_mean.cpu(), response)\n",
    "plt.axline((0.4, 0.4), (1,1), c='r')\n",
    "plt.xlabel('True')\n",
    "plt.ylabel('Predict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  9.,  19.,  19.,  24.,  42.,  50., 113., 180., 283., 285.]),\n",
       " array([0.2304    , 0.30735999, 0.38431999, 0.46127999, 0.53824002,\n",
       "        0.61519998, 0.69216001, 0.76911998, 0.84608001, 0.92303997,\n",
       "        1.        ]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "general",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

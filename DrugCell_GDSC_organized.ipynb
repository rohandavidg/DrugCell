{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/lambda_stor/homes/ac.tfeng/git/DrugCell'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/homes/ac.tfeng/miniconda3/envs/general/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# import anndata\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List, Union\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.distributions import MultivariateNormal\n",
    "\n",
    "import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"4\"\n",
    "\n",
    "import copy\n",
    "\n",
    "import tqdm\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from codes.utils.util import *\n",
    "from codes.drugcell_NN import *\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "  DEVICE = 'cuda'\n",
    "else:\n",
    "  DEVICE = 'cpu'\n",
    "  \n",
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Feb 11 12:53:59 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.104.12             Driver Version: 535.104.12   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  Tesla V100-SXM2-32GB           On  | 00000000:1A:00.0 Off |                    0 |\n",
      "| N/A   52C    P0             236W / 300W |  18195MiB / 32768MiB |     97%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla V100-SXM2-32GB           On  | 00000000:1B:00.0 Off |                    0 |\n",
      "| N/A   38C    P0              56W / 300W |    313MiB / 32768MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   2  Tesla V100-SXM2-32GB           On  | 00000000:3D:00.0 Off |                    0 |\n",
      "| N/A   41C    P0             112W / 300W |   1685MiB / 32768MiB |     33%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   3  Tesla V100-SXM2-32GB           On  | 00000000:3E:00.0 Off |                    0 |\n",
      "| N/A   33C    P0             112W / 300W |   1685MiB / 32768MiB |     33%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   4  Tesla V100-SXM2-32GB           On  | 00000000:88:00.0 Off |                    0 |\n",
      "| N/A   34C    P0              83W / 300W |   1705MiB / 32768MiB |     33%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   5  Tesla V100-SXM2-32GB           On  | 00000000:89:00.0 Off |                    0 |\n",
      "| N/A   35C    P0              58W / 300W |    313MiB / 32768MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   6  Tesla V100-SXM2-32GB           On  | 00000000:B2:00.0 Off |                    0 |\n",
      "| N/A   31C    P0              55W / 300W |    313MiB / 32768MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   7  Tesla V100-SXM2-32GB           On  | 00000000:B3:00.0 Off |                    0 |\n",
      "| N/A   28C    P0              57W / 300W |    313MiB / 32768MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A     24277      C   .../miniconda3/envs/ddmd/bin/python3.9      620MiB |\n",
      "|    0   N/A  N/A    620417      C   python                                    16706MiB |\n",
      "|    0   N/A  N/A   3313300      C   python                                      864MiB |\n",
      "|    1   N/A  N/A    620417      C   python                                      308MiB |\n",
      "|    2   N/A  N/A    620417      C   python                                      308MiB |\n",
      "|    2   N/A  N/A   1098213      C   python                                     1372MiB |\n",
      "|    3   N/A  N/A    620417      C   python                                      308MiB |\n",
      "|    3   N/A  N/A    664536      C   python                                     1372MiB |\n",
      "|    4   N/A  N/A    620417      C   python                                      308MiB |\n",
      "|    4   N/A  N/A    867737      C   python                                     1392MiB |\n",
      "|    5   N/A  N/A    620417      C   python                                      308MiB |\n",
      "|    6   N/A  N/A    620417      C   python                                      308MiB |\n",
      "|    7   N/A  N/A    620417      C   python                                      308MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = 'cuda:6'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GDSC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_col_names_in_multilevel_dataframe(\n",
    "    df: pd.DataFrame,\n",
    "    level_map: dict,\n",
    "    gene_system_identifier: Union[str, List[str]]=\"Gene_Symbol\") -> pd.DataFrame:\n",
    "    \"\"\" Util function that supports loading of the omic data files.\n",
    "    Returns the input dataframe with the multi-level column names renamed as\n",
    "    specified by the gene_system_identifier arg.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): omics dataframe\n",
    "        level_map (dict): encodes the column level and the corresponding identifier systems\n",
    "        gene_system_identifier (str or list of str): gene identifier system to use\n",
    "            options: \"Entrez\", \"Gene_Symbol\", \"Ensembl\", \"all\", or any list\n",
    "                     combination of [\"Entrez\", \"Gene_Symbol\", \"Ensembl\"]\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: the input dataframe with the specified multi-level column names\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    level_names = list(level_map.keys())\n",
    "    level_values = list(level_map.values())\n",
    "    n_levels = len(level_names)\n",
    "    \n",
    "    if isinstance(gene_system_identifier, list) and len(gene_system_identifier) == 1:\n",
    "        gene_system_identifier = gene_system_identifier[0]\n",
    "\n",
    "    # print(gene_system_identifier)\n",
    "    # import pdb; pdb.set_trace()\n",
    "    if isinstance(gene_system_identifier, str):\n",
    "        if gene_system_identifier == \"all\":\n",
    "            df.columns = df.columns.rename(level_names, level=level_values)  # assign multi-level col names\n",
    "        else:\n",
    "            df.columns = df.columns.get_level_values(level_map[gene_system_identifier])  # retian specific column level\n",
    "    else:\n",
    "        assert len(gene_system_identifier) <= n_levels, f\"'gene_system_identifier' can't contain more than {n_levels} items.\"\n",
    "        set_diff = list(set(gene_system_identifier).difference(set(level_names)))\n",
    "        assert len(set_diff) == 0, f\"Passed unknown gene identifiers: {set_diff}\"\n",
    "        kk = {i: level_map[i] for i in level_map if i in gene_system_identifier}\n",
    "        # print(list(kk.keys()))\n",
    "        # print(list(kk.values()))\n",
    "        df.columns = df.columns.rename(list(kk.keys()), level=kk.values())  # assign multi-level col names\n",
    "        drop_levels = list(set(level_map.values()).difference(set(kk.values())))\n",
    "        df = df.droplevel(level=drop_levels, axis=1)\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_gene_expression_data(gene_expression_file_path, \n",
    "    gene_system_identifier: Union[str, List[str]]=\"Gene_Symbol\",\n",
    "    sep: str=\"\\t\",\n",
    "    verbose: bool=True) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Returns gene expression data.\n",
    "\n",
    "    Args:\n",
    "        gene_system_identifier (str or list of str): gene identifier system to use\n",
    "            options: \"Entrez\", \"Gene_Symbol\", \"Ensembl\", \"all\", or any list\n",
    "                     combination of [\"Entrez\", \"Gene_Symbol\", \"Ensembl\"]\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: dataframe with the omic data\n",
    "    \"\"\"\n",
    "    # level_map encodes the relationship btw the column and gene identifier system\n",
    "    level_map = {\"Ensembl\": 0, \"Entrez\": 1, \"Gene_Symbol\": 2}\n",
    "    header = [i for i in range(len(level_map))]\n",
    "\n",
    "    df = pd.read_csv(gene_expression_file_path, sep=sep, index_col=0, header=header)\n",
    "\n",
    "    df.index.name = \"improve_sample_id\"  # assign index name\n",
    "    df = set_col_names_in_multilevel_dataframe(df, level_map, gene_system_identifier)\n",
    "    if verbose:\n",
    "        print(f\"Gene expression data: {df.shape}\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gene expression data: (1007, 30805)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1252376/2783280313.py:2: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  response = pd.read_csv(\"data/GDSC/response.tsv\", sep = '\\t')\n"
     ]
    }
   ],
   "source": [
    "gene_express = load_gene_expression_data(\"data/GDSC/cancer_gene_expression.tsv\")\n",
    "response = pd.read_csv(\"data/GDSC/response.tsv\", sep = '\\t')\n",
    "\n",
    "gdsc_info = pd.read_csv(\"data/GDSC/GDSC_info.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "drug_ecfp4_nbits512 = pd.read_csv(\"data/GDSC/drug_ecfp4_nbits512.tsv\", sep = '\\t', index_col=0)\n",
    "drug_tensor = torch.tensor(drug_ecfp4_nbits512.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdsc = pd.merge(gene_express, gdsc_info, how=\"inner\", left_on=[\"improve_sample_id\"], right_on=['ModelID'])\n",
    "gdsc_x = gdsc.loc[:, gene_express.columns]\n",
    "gdsc_y = gdsc.loc[:, 'DepmapModelType']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gene ontology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of cell lines = 1225\n",
      "Total number of drugs = 684\n",
      "There are 3008 genes\n",
      "There are 1 roots: GO:0008150\n",
      "There are 2086 terms\n",
      "There are 1 connected componenets\n"
     ]
    }
   ],
   "source": [
    "training_file = \"data/drugcell_train.txt\"\n",
    "testing_file = \"data/drugcell_test.txt\"\n",
    "val_file = \"data/drugcell_val.txt\"\n",
    "cell2id_file = \"data/cell2ind.txt\"\n",
    "drug2id_file = \"data/drug2ind.txt\"\n",
    "genotype_file = \"data/cell2mutation.txt\"\n",
    "fingerprint_file = \"data/drug2fingerprint.txt\"\n",
    "onto_file = \"data/drugcell_ont.txt\"\n",
    "gene2id_file = \"data/gene2ind.txt\"\n",
    "\n",
    "train_data, feature_dict, cell2id_mapping, drug2id_mapping = prepare_train_data(training_file, \n",
    "                                                                  testing_file, cell2id_file, \n",
    "                                                                  drug2id_file)\n",
    "\n",
    "gene2id_mapping = load_mapping(gene2id_file)\n",
    "\n",
    "# load cell/drug features\n",
    "cell_features = np.genfromtxt(genotype_file, delimiter=',')\n",
    "drug_features = np.genfromtxt(fingerprint_file, delimiter=',')\n",
    "\n",
    "num_cells = len(cell2id_mapping)\n",
    "num_drugs = len(drug2id_mapping)\n",
    "num_genes = len(gene2id_mapping)\n",
    "drug_dim = len(drug_features[0,:])\n",
    "\n",
    "# load ontology\n",
    "dG, root, term_size_map, \\\n",
    "    term_direct_gene_map = load_ontology(onto_file, \n",
    "                                         gene2id_mapping)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Align gene id with GDCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancer_2_idx = {}\n",
    "idx_2_cancer = {}\n",
    "cancer_type_idx = []\n",
    "\n",
    "i = 0\n",
    "for cancer in gdsc_y:\n",
    "    if cancer not in cancer_2_idx:\n",
    "        cancer_2_idx[cancer] = i\n",
    "        idx_2_cancer[i] = cancer\n",
    "        cancer_type_idx.append(i)\n",
    "        \n",
    "        i += 1\n",
    "    else:\n",
    "        cancer_type_idx.append(cancer_2_idx[cancer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_intersect_list = list(set(gene2id_mapping.keys()) & set(gene_express.columns))\n",
    "gdsc_tensor = torch.zeros(gene_express.shape[0], num_genes)\n",
    "for gene in gene_intersect_list:\n",
    "    idx = gene2id_mapping[gene]\n",
    "    gdsc_tensor[:,idx] = torch.tensor(gene_express[gene])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdsc_row_key_id = {k: v for v, k in enumerate(gene_express.index)}\n",
    "gdsc_row_id_key = {v: k for v, k in enumerate(gene_express.index)}\n",
    "\n",
    "chem_row_key_id = {k: v for v, k in enumerate(drug_ecfp4_nbits512.index)}\n",
    "chem_row_id_key = {v: k for v, k in enumerate(drug_ecfp4_nbits512.index)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_gdcs2 = response[response['source'] == 'GDSCv2'].loc[:,['improve_sample_id', \n",
    "                                                             'improve_chem_id',\n",
    "                                                             'auc']]\n",
    "response_gdcs2 = response_gdcs2.replace({'improve_sample_id': gdsc_row_key_id})\n",
    "response_gdcs2 = response_gdcs2.replace({'improve_chem_id': chem_row_key_id})\n",
    "response_gdcs2 = torch.tensor(response_gdcs2.to_numpy())\n",
    "\n",
    "class GDSCData(Dataset):\n",
    "    \n",
    "    def __init__(self, response, gene_tensor, chem_tensor):\n",
    "        self.response = response\n",
    "        self.gene_tensor = gene_tensor\n",
    "        self.chem_tensor = chem_tensor\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.response.shape[0]\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        sample = self.response[index,:]\n",
    "        \n",
    "        X_gene = self.gene_tensor[sample[0].long() ,:]\n",
    "        X_chem = self.chem_tensor[sample[1].long() ,:]\n",
    "        \n",
    "        y = sample[2]\n",
    "\n",
    "        X = torch.cat((X_gene, X_chem), 0)\n",
    "        \n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gdcs_idx = torch.unique(response_gdcs2[:,0], sorted=False)[:423]\n",
    "test_gdcs_idx = torch.unique(response_gdcs2[:,0], sorted=False)[423:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdsc_data = GDSCData(response_gdcs2, gdsc_tensor, drug_tensor)\n",
    "gdsc_data_train = GDSCData(response_gdcs2[torch.isin(response_gdcs2[:,0], train_gdcs_idx)].float(), gdsc_tensor, drug_tensor)\n",
    "gdsc_data_test = GDSCData(response_gdcs2[torch.isin(response_gdcs2[:,0], test_gdcs_idx)].float(), gdsc_tensor, drug_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(gdsc_data_train, batch_size=8192, shuffle=True)\n",
    "test_loader = DataLoader(gdsc_data_test, batch_size=8192, shuffle=False)\n",
    "(inputdata, response) = next(iter(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"hpo_data/response_gdcs2.csv\", response_gdcs2.numpy(), delimiter=\",\")\n",
    "np.savetxt(\"hpo_data/gdsc_tensor.csv\", gdsc_tensor.numpy(), delimiter=\",\")\n",
    "np.savetxt(\"hpo_data/drug_tensor.csv\", drug_tensor.numpy(), delimiter=\",\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[9.4900e+02, 6.7400e+02, 5.8330e-01],\n",
       "        [9.4900e+02, 6.7400e+02, 6.0260e-01],\n",
       "        [9.4900e+02, 6.7400e+02, 4.3030e-01],\n",
       "        ...,\n",
       "        [5.1500e+02, 2.1400e+02, 9.1050e-01],\n",
       "        [5.1500e+02, 1.4430e+03, 9.5660e-01],\n",
       "        [5.1500e+02, 8.5500e+02, 8.4260e-01]], dtype=torch.float64)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_gdcs2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import float32\n",
    "\n",
    "\n",
    "arr = np.loadtxt(\"hpo_data/response_gdcs2.csv\",\n",
    "                 delimiter=\",\", dtype=float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.490e+02, 6.740e+02, 5.833e-01],\n",
       "       [9.490e+02, 6.740e+02, 6.026e-01],\n",
       "       [9.490e+02, 6.740e+02, 4.303e-01],\n",
       "       ...,\n",
       "       [5.150e+02, 2.140e+02, 9.105e-01],\n",
       "       [5.150e+02, 1.443e+03, 9.566e-01],\n",
       "       [5.150e+02, 8.550e+02, 8.426e-01]], dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Drugcell_Vae(nn.Module):\n",
    "\n",
    "    def __init__(self, term_size_map, term_direct_gene_map, dG, ngene, ndrug, root, \n",
    "                 num_hiddens_genotype, num_hiddens_drug, num_hiddens_final, \n",
    "                 n_class, inter_loss_penalty = 0.2):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.root = root\n",
    "        self.num_hiddens_genotype = num_hiddens_genotype\n",
    "        self.num_hiddens_drug = num_hiddens_drug\n",
    "        \n",
    "        \n",
    "        self.num_hiddens_final = num_hiddens_final\n",
    "        self.n_class = n_class\n",
    "        self.inter_loss_penalty = inter_loss_penalty\n",
    "        self.dG = copy.deepcopy(dG)\n",
    "\n",
    "        # dictionary from terms to genes directly annotated with the term\n",
    "        self.term_direct_gene_map = term_direct_gene_map\n",
    "\n",
    "        self.term_visit_count = {}\n",
    "        self.init_term_visits(term_size_map)\n",
    "        \n",
    "        # calculate the number of values in a state (term): term_size_map is the number of all genes annotated with the term\n",
    "        self.term_dim_map = {}\n",
    "        self.cal_term_dim(term_size_map)\n",
    "\n",
    "        # ngenes, gene_dim are the number of all genes\n",
    "        self.gene_dim = ngene\n",
    "        self.drug_dim = ndrug\n",
    "\n",
    "        # add modules for neural networks to process genotypes\n",
    "        self.contruct_direct_gene_layer()\n",
    "        self.construct_NN_graph(self.dG)\n",
    "\n",
    "        # add modules for neural networks to process drugs\n",
    "        self.construct_NN_drug()\n",
    "\n",
    "        # add modules for final layer TODO: modify it into VAE\n",
    "        final_input_size = num_hiddens_genotype + num_hiddens_drug[-1]\n",
    "        self.add_module('final_linear_layer', nn.Linear(final_input_size, num_hiddens_final * 2))\n",
    "        self.add_module('final_batchnorm_layer', nn.BatchNorm1d(num_hiddens_final * 2))\n",
    "        self.add_module('final_aux_linear_layer', nn.Linear(num_hiddens_final * 2, 1))\n",
    "        self.add_module('final_linear_layer_output', nn.Linear(1, 1))\n",
    "        \n",
    "        self.decoder_affine = nn.Linear(num_hiddens_final, 1)\n",
    "\n",
    "    def init_term_visits(self, term_size_map):\n",
    "        \n",
    "        for term in term_size_map:\n",
    "            self.term_visit_count[term] = 0\n",
    "    \n",
    "    # calculate the number of values in a state (term)\n",
    "    def cal_term_dim(self, term_size_map):\n",
    "\n",
    "        for term, term_size in term_size_map.items():\n",
    "            num_output = self.num_hiddens_genotype\n",
    "\n",
    "            # log the number of hidden variables per each term\n",
    "            num_output = int(num_output)\n",
    "#            print(\"term\\t%s\\tterm_size\\t%d\\tnum_hiddens\\t%d\" % (term, term_size, num_output))\n",
    "            self.term_dim_map[term] = num_output\n",
    "\n",
    "\n",
    "    # build a layer for forwarding gene that are directly annotated with the term\n",
    "    def contruct_direct_gene_layer(self):\n",
    "\n",
    "        for term, gene_set in self.term_direct_gene_map.items():\n",
    "            if len(gene_set) == 0:\n",
    "                print('There are no directed asscoiated genes for', term)\n",
    "                sys.exit(1)\n",
    "\n",
    "            # if there are some genes directly annotated with the term, add a layer taking in all genes and forwarding out only those genes\n",
    "            self.add_module(term+'_direct_gene_layer', nn.Linear(self.gene_dim, len(gene_set)))\n",
    "\n",
    "\n",
    "    # add modules for fully connected neural networks for drug processing\n",
    "    def construct_NN_drug(self):\n",
    "        input_size = self.drug_dim\n",
    "\n",
    "        for i in range(len(self.num_hiddens_drug)):\n",
    "            self.add_module('drug_linear_layer_' + str(i+1), nn.Linear(input_size, self.num_hiddens_drug[i]))\n",
    "            self.add_module('drug_batchnorm_layer_' + str(i+1), nn.BatchNorm1d(self.num_hiddens_drug[i]))\n",
    "            self.add_module('drug_aux_linear_layer1_' + str(i+1), nn.Linear(self.num_hiddens_drug[i],1))\n",
    "            self.add_module('drug_aux_linear_layer2_' + str(i+1), nn.Linear(1,1))\n",
    "\n",
    "            input_size = self.num_hiddens_drug[i]\n",
    "\n",
    "    # start from bottom (leaves), and start building a neural network using the given ontology\n",
    "    # adding modules --- the modules are not connected yet\n",
    "    def construct_NN_graph(self, dG):\n",
    "\n",
    "        self.term_layer_list = []   # term_layer_list stores the built neural network\n",
    "        self.term_neighbor_map = {}\n",
    "\n",
    "        # term_neighbor_map records all children of each term\n",
    "        for term in dG.nodes():\n",
    "            self.term_neighbor_map[term] = []\n",
    "            for child in dG.neighbors(term):\n",
    "                self.term_neighbor_map[term].append(child)\n",
    "\n",
    "        while True:\n",
    "            leaves = [n for n in dG.nodes() if dG.out_degree(n) == 0]\n",
    "            #leaves = [n for n,d in dG.out_degree().items() if d==0]\n",
    "            #leaves = [n for n,d in dG.out_degree() if d==0]\n",
    "\n",
    "            if len(leaves) == 0:\n",
    "                break\n",
    "\n",
    "            self.term_layer_list.append(leaves)\n",
    "\n",
    "            for term in leaves:\n",
    "\n",
    "                # input size will be #chilren + #genes directly annotated by the term\n",
    "                input_size = 0\n",
    "\n",
    "                for child in self.term_neighbor_map[term]:\n",
    "                    input_size += self.term_dim_map[child]\n",
    "\n",
    "                if term in self.term_direct_gene_map:\n",
    "                    input_size += len(self.term_direct_gene_map[term])\n",
    "\n",
    "                # term_hidden is the number of the hidden variables in each state\n",
    "                term_hidden = self.term_dim_map[term]\n",
    "\n",
    "                self.add_module(term+'_linear_layer', nn.Linear(input_size, term_hidden))\n",
    "                self.add_module(term+'_batchnorm_layer', nn.BatchNorm1d(term_hidden))\n",
    "                self.add_module(term+'_aux_linear_layer1', nn.Linear(term_hidden, term_hidden))\n",
    "                self.add_module(term+'_aux_linear_layer2', nn.Linear(term_hidden, 1))\n",
    "\n",
    "            dG.remove_nodes_from(leaves)\n",
    "\n",
    "\n",
    "    # definition of encoder\n",
    "    def encoder(self, x):\n",
    "        gene_input = x.narrow(1, 0, self.gene_dim)\n",
    "        drug_input = x.narrow(1, self.gene_dim, self.drug_dim)\n",
    "        \n",
    "        # define forward function for genotype dcell #############################################\n",
    "        term_gene_out_map = {}\n",
    "\n",
    "        for term, _ in self.term_direct_gene_map.items():\n",
    "            term_gene_out_map[term] = self._modules[term + '_direct_gene_layer'](gene_input)\n",
    "\n",
    "        term_NN_out_map = {}\n",
    "        aux_out_map = {}\n",
    "\n",
    "        for i, layer in enumerate(self.term_layer_list):\n",
    "\n",
    "            for term in layer:\n",
    "\n",
    "                child_input_list = []\n",
    "\n",
    "                self.term_visit_count[term] += 1\n",
    "                \n",
    "                for child in self.term_neighbor_map[term]:\n",
    "                    child_input_list.append(term_NN_out_map[child])\n",
    "\n",
    "                if term in self.term_direct_gene_map:\n",
    "                    child_input_list.append(term_gene_out_map[term])\n",
    "\n",
    "                child_input = torch.cat(child_input_list,1)\n",
    "\n",
    "                term_NN_out = self._modules[term+'_linear_layer'](child_input)\n",
    "\n",
    "                Tanh_out = torch.tanh(term_NN_out)\n",
    "                term_NN_out_map[term] = self._modules[term+'_batchnorm_layer'](Tanh_out)\n",
    "                aux_layer1_out = torch.tanh(self._modules[term+'_aux_linear_layer1'](term_NN_out_map[term]))\n",
    "                aux_out_map[term] = self._modules[term+'_aux_linear_layer2'](aux_layer1_out)\n",
    "\n",
    "        drug_out = drug_input\n",
    "\n",
    "        for i in range(1, len(self.num_hiddens_drug)+1, 1):\n",
    "            drug_out = self._modules['drug_batchnorm_layer_'+str(i)]( torch.tanh(self._modules['drug_linear_layer_' + str(i)](drug_out)))\n",
    "            term_NN_out_map['drug_'+str(i)] = drug_out\n",
    "\n",
    "            aux_layer1_out = torch.tanh(self._modules['drug_aux_linear_layer1_'+str(i)](drug_out))\n",
    "            aux_out_map['drug_'+str(i)] = self._modules['drug_aux_linear_layer2_'+str(i)](aux_layer1_out)\n",
    "\n",
    "\n",
    "        # connect two neural networks at the top #################################################\n",
    "        final_input = torch.cat((term_NN_out_map[self.root], drug_out), 1)\n",
    "\n",
    "        out = self._modules['final_batchnorm_layer'](torch.tanh(self._modules['final_linear_layer'](final_input)))\n",
    "        term_NN_out_map['final'] = out\n",
    "\n",
    "        aux_layer_out = torch.tanh(self._modules['final_aux_linear_layer'](out))\n",
    "        aux_out_map['final'] = self._modules['final_linear_layer_output'](aux_layer_out)\n",
    "\n",
    "        return aux_out_map, term_NN_out_map\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        aux_out_map, term_NN_out_map = self.encoder(x)\n",
    "        \n",
    "        mu = term_NN_out_map['final'][..., :self.num_hiddens_final]\n",
    "        log_var = term_NN_out_map['final'][..., :self.num_hiddens_final]  # T X batch X z_dim\n",
    "        std_dec = log_var.mul(0.5).exp_()\n",
    "        # std_dec = 1\n",
    "        \n",
    "        latent = MultivariateNormal(loc = mu, \n",
    "                                    scale_tril=torch.diag_embed(std_dec))\n",
    "        z = latent.rsample()\n",
    "        \n",
    "        recon_mean = self.decoder_affine(z)\n",
    "        recon_mean = F.sigmoid(recon_mean)\n",
    "\n",
    "        return recon_mean, mu, log_var, aux_out_map, term_NN_out_map\n",
    "    \n",
    "    def loss_log_vae(self, recon_mean, y, mu, log_var, beta = 0.001):\n",
    "        # y: true labels\n",
    "        ori_y_shape = y.shape\n",
    "        \n",
    "        class_loss = F.mse_loss(recon_mean.view(-1), \n",
    "                                     y.reshape(-1), reduction = 'none').div(np.log(2)).view(*ori_y_shape)\n",
    "        \n",
    "        KLD = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp(), \n",
    "                              dim = -1)\n",
    "        \n",
    "        log_loss = class_loss + beta * KLD\n",
    "        log_loss = torch.mean(torch.logsumexp(log_loss, 0))\n",
    "        \n",
    "        return log_loss\n",
    "    \n",
    "    def intermediate_loss(self, aux_out_map, y):\n",
    "        \n",
    "        inter_loss = 0\n",
    "        for name, output in aux_out_map.items():\n",
    "            if name == 'final':\n",
    "                inter_loss += 0\n",
    "            else: # change 0.2 to smaller one for big terms\n",
    "                ori_y_shape = y.shape\n",
    "        \n",
    "                term_loss = F.mse_loss(output.view(-1), \n",
    "                                             y.reshape(-1), \n",
    "                                             reduction = 'none').div(np.log(2)).view(*ori_y_shape)\n",
    "                inter_loss += term_loss\n",
    "\n",
    "        return inter_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_term_mask(term_direct_gene_map, gene_dim, device):\n",
    "\n",
    "    term_mask_map = {}\n",
    "\n",
    "    for term, gene_set in term_direct_gene_map.items():\n",
    "\n",
    "        mask = torch.zeros(len(gene_set), gene_dim)\n",
    "\n",
    "        for i, gene_id in enumerate(gene_set):\n",
    "            mask[i, gene_id] = 1\n",
    "\n",
    "        mask_gpu = torch.autograd.Variable(mask)\n",
    "\n",
    "        term_mask_map[term] = mask_gpu.to(device)\n",
    "\n",
    "    return term_mask_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drug_ecfp4_nbits512.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "187it [10:12,  3.27s/it, Epoch=0, Training Loss=17.8, Testing Loss=0.0617]\n",
      "187it [10:08,  3.25s/it, Epoch=1, Training Loss=23.1, Testing Loss=0.0301]\n",
      "187it [10:08,  3.26s/it, Epoch=2, Training Loss=18.1, Testing Loss=0.022] \n",
      "187it [10:32,  3.38s/it, Epoch=3, Training Loss=18, Testing Loss=0.0191]  \n",
      "187it [10:41,  3.43s/it, Epoch=4, Training Loss=18.7, Testing Loss=0.0171]\n",
      "187it [10:12,  3.27s/it, Epoch=5, Training Loss=18.9, Testing Loss=0.0159]\n",
      "187it [10:09,  3.26s/it, Epoch=6, Training Loss=16.5, Testing Loss=0.0149]\n",
      "187it [10:10,  3.26s/it, Epoch=7, Training Loss=23.8, Testing Loss=0.0153]\n",
      "187it [10:08,  3.25s/it, Epoch=8, Training Loss=18.2, Testing Loss=0.0144]\n",
      "187it [10:09,  3.26s/it, Epoch=9, Training Loss=18, Testing Loss=0.015]   \n",
      "187it [10:07,  3.25s/it, Epoch=10, Training Loss=22.2, Testing Loss=0.0142]\n",
      "187it [10:08,  3.25s/it, Epoch=11, Training Loss=20.1, Testing Loss=0.014] \n",
      "187it [10:10,  3.26s/it, Epoch=12, Training Loss=17.3, Testing Loss=0.0147]\n",
      "187it [10:14,  3.28s/it, Epoch=13, Training Loss=16.3, Testing Loss=0.0138]\n",
      "187it [10:16,  3.30s/it, Epoch=14, Training Loss=18.4, Testing Loss=0.0139]\n",
      "187it [10:10,  3.26s/it, Epoch=15, Training Loss=16.1, Testing Loss=0.0139]\n",
      "187it [10:09,  3.26s/it, Epoch=16, Training Loss=16.3, Testing Loss=0.0136]\n",
      "187it [10:07,  3.25s/it, Epoch=17, Training Loss=17.3, Testing Loss=0.0138]\n",
      "187it [10:09,  3.26s/it, Epoch=18, Training Loss=18.9, Testing Loss=0.0134]\n",
      "187it [10:10,  3.26s/it, Epoch=19, Training Loss=18.8, Testing Loss=0.0134]\n",
      "187it [10:10,  3.26s/it, Epoch=20, Training Loss=17.4, Testing Loss=0.0135]\n",
      "187it [10:08,  3.26s/it, Epoch=21, Training Loss=20.6, Testing Loss=0.013] \n",
      "187it [10:06,  3.24s/it, Epoch=22, Training Loss=16.9, Testing Loss=0.0133]\n",
      "187it [10:09,  3.26s/it, Epoch=23, Training Loss=15.3, Testing Loss=0.0133]\n",
      "187it [10:09,  3.26s/it, Epoch=24, Training Loss=18.1, Testing Loss=0.0134]\n",
      "187it [10:16,  3.30s/it, Epoch=25, Training Loss=18.1, Testing Loss=0.0134]\n",
      "187it [10:10,  3.27s/it, Epoch=26, Training Loss=19.9, Testing Loss=0.0125]\n",
      "187it [10:14,  3.28s/it, Epoch=27, Training Loss=19.4, Testing Loss=0.0131]\n",
      "187it [10:09,  3.26s/it, Epoch=28, Training Loss=17.7, Testing Loss=0.0129]\n",
      "187it [10:11,  3.27s/it, Epoch=29, Training Loss=17.4, Testing Loss=0.0132]\n",
      "187it [10:12,  3.27s/it, Epoch=30, Training Loss=18.7, Testing Loss=0.0128]\n",
      "187it [10:09,  3.26s/it, Epoch=31, Training Loss=18.2, Testing Loss=0.0131]\n",
      "187it [10:10,  3.26s/it, Epoch=32, Training Loss=13.6, Testing Loss=0.013] \n",
      "187it [10:12,  3.28s/it, Epoch=33, Training Loss=16.4, Testing Loss=0.013] \n",
      "187it [10:11,  3.27s/it, Epoch=34, Training Loss=18.7, Testing Loss=0.0127]\n",
      "187it [10:09,  3.26s/it, Epoch=35, Training Loss=19.6, Testing Loss=0.0129]\n",
      "187it [10:09,  3.26s/it, Epoch=36, Training Loss=18.5, Testing Loss=0.0128]\n",
      "187it [10:09,  3.26s/it, Epoch=37, Training Loss=21.5, Testing Loss=0.0136]\n",
      "187it [10:08,  3.25s/it, Epoch=38, Training Loss=18.8, Testing Loss=0.013] \n",
      "187it [10:11,  3.27s/it, Epoch=39, Training Loss=19, Testing Loss=0.0126]  \n",
      "187it [10:12,  3.27s/it, Epoch=40, Training Loss=16.1, Testing Loss=0.0126]\n",
      "187it [10:12,  3.27s/it, Epoch=41, Training Loss=22.4, Testing Loss=0.0132]\n",
      "187it [10:14,  3.28s/it, Epoch=42, Training Loss=17.9, Testing Loss=0.0128]\n",
      "187it [10:13,  3.28s/it, Epoch=43, Training Loss=19.7, Testing Loss=0.013] \n",
      "187it [10:11,  3.27s/it, Epoch=44, Training Loss=23.1, Testing Loss=0.0131]\n",
      "187it [10:05,  3.24s/it, Epoch=45, Training Loss=19.7, Testing Loss=0.0127]\n",
      "187it [10:06,  3.24s/it, Epoch=46, Training Loss=16.2, Testing Loss=0.0126]\n",
      "187it [10:08,  3.25s/it, Epoch=47, Training Loss=18.8, Testing Loss=0.0129]\n",
      "187it [10:09,  3.26s/it, Epoch=48, Training Loss=17.7, Testing Loss=0.0128]\n",
      "187it [10:07,  3.25s/it, Epoch=49, Training Loss=18.8, Testing Loss=0.0125]\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1)\n",
    "\n",
    "num_hiddens_genotype = 6\n",
    "num_hiddens_final = 6\n",
    "drug_hiddens='100,50,6'\n",
    "num_hiddens_drug = list(map(int, drug_hiddens.split(',')))\n",
    "# num_hiddens_drug = 6\n",
    "num_drugs = drug_ecfp4_nbits512.shape[1]\n",
    "\n",
    "model = Drugcell_Vae(term_size_map, term_direct_gene_map, dG, num_genes, num_drugs, \n",
    "                 root, num_hiddens_genotype, num_hiddens_drug, num_hiddens_final, \n",
    "                 n_class = len(cancer_2_idx))\n",
    "model.to(DEVICE)\n",
    "term_mask_map = create_term_mask(model.term_direct_gene_map, num_genes, device = DEVICE)\n",
    "\n",
    "\n",
    "learning_rate = 0.001\n",
    "torch.manual_seed(0)\n",
    "training_loss_list = []\n",
    "testing_loss_list = []\n",
    "epoch_list = []\n",
    "accu_list = []\n",
    "train_epochs = 50\n",
    "best_loss = 1000\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, betas=(0.9, 0.99), eps=1e-05)\n",
    "term_mask_map = create_term_mask(model.term_direct_gene_map, gene_dim=num_genes, device=DEVICE)\n",
    "\n",
    "optimizer.zero_grad()\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    term_name = name.split('_')[0]\n",
    "\n",
    "    if '_direct_gene_layer.weight' in name:\n",
    "        param.data = torch.mul(param.data, term_mask_map[term_name].to(DEVICE)) * 0.1\n",
    "    else:\n",
    "        param.data = param.data * 0.1\n",
    "\n",
    "mse_tmp_testing = torch.tensor(0, device=DEVICE)\n",
    "# tepoch = tqdm.tqdm(range(train_epochs))\n",
    "for epoch in range(train_epochs):\n",
    "    # Train\n",
    "    model.train()\n",
    "    train_predict = torch.zeros(0, 0).to(DEVICE)\n",
    "\n",
    "    tloader = tqdm.tqdm(enumerate(train_loader))\n",
    "    for i, (data, response) in tloader:\n",
    "        # Convert torch tensor to Variable\n",
    "\n",
    "        # Forward + Backward + Optimize\n",
    "        optimizer.zero_grad()  # zero the gradient buffer\n",
    "\n",
    "        # Here term_NN_out_map is a dictionary\n",
    "        recon_mean, mu, log_var, aux_out_map, term_NN_out_map = model(data.to(DEVICE))\n",
    "\n",
    "        if train_predict.size()[0] == 0:\n",
    "            train_predict = aux_out_map[\"final\"].data\n",
    "        else:\n",
    "            train_predict = torch.cat([train_predict, aux_out_map[\"final\"].data], dim=0)\n",
    "\n",
    "        total_loss = 0\n",
    "\n",
    "        loss_vae = model.loss_log_vae(\n",
    "            recon_mean=recon_mean, y=response.to(DEVICE), mu=mu, log_var=log_var, beta=0.001\n",
    "        )\n",
    "\n",
    "        loss_intermidiate = model.intermediate_loss(aux_out_map, response.to(DEVICE))\n",
    "\n",
    "        total_loss = torch.mean(loss_vae + model.inter_loss_penalty * loss_intermidiate)\n",
    "        \n",
    "        tmp_loss = total_loss.item()\n",
    "        # tepoch.set_postfix({\"Epoch\": epoch, \n",
    "        #                     \"Training Loss\": tmp_loss, \n",
    "        #                     \"Testing Loss\": mse_tmp_testing.item()})\n",
    "        \n",
    "        total_loss.backward()\n",
    "\n",
    "        for name, param in model.named_parameters():\n",
    "            if \"_direct_gene_layer.weight\" not in name:\n",
    "                continue\n",
    "            term_name = name.split(\"_\")[0]\n",
    "            # print name, param.grad.data.size(), term_mask_map[term_name].size()\n",
    "            param.grad.data = torch.mul(param.grad.data, term_mask_map[term_name])\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % 6 == 0:\n",
    "            with torch.no_grad():\n",
    "                (inputdata, response) = next(iter(test_loader))\n",
    "                recon_mean, mu, log_var, aux_out_map, term_NN_out_map = model(inputdata.to(DEVICE))\n",
    "\n",
    "                mse_tmp_testing = F.mse_loss(recon_mean.detach().squeeze().cpu(), response.squeeze())\n",
    "\n",
    "                tloader.set_postfix({\"Epoch\": epoch, \n",
    "                                    \"Training Loss\": tmp_loss, \n",
    "                                    \"Testing Loss\": mse_tmp_testing.item()})\n",
    "                \n",
    "                training_loss_list.append(tmp_loss)\n",
    "                testing_loss_list.append(mse_tmp_testing.item())\n",
    "                epoch_list.append(epoch)\n",
    "    with torch.no_grad():\n",
    "        (inputdata, response) = next(iter(test_loader))\n",
    "        recon_mean, mu, log_var, aux_out_map, term_NN_out_map = model(inputdata.to(DEVICE))\n",
    "\n",
    "        mse_tmp_testing = F.mse_loss(recon_mean.detach().squeeze().cpu(), response.squeeze())\n",
    "        \n",
    "        if mse_tmp_testing < best_loss:\n",
    "            torch.save(model, \"gdsc_drug_epoch_new.pt\")\n",
    "    # if epoch % 10 == 0:\n",
    "    # torch.save(model, \"gdsc_50.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "model= torch.load(\"gdsc_drug_epoch_new.pt\", map_location=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/homes/ac.tfeng/miniconda3/envs/general/lib/python3.9/site-packages/torch/nn/functional.py:1967: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    (inputdata, response) = next(iter(test_loader))\n",
    "    recon_mean, mu, log_var, aux_out_map, term_NN_out_map = model(inputdata.to(DEVICE))\n",
    "\n",
    "    mse_tmp_testing = F.mse_loss(recon_mean.detach().squeeze().cpu(), response.squeeze())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "general",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
